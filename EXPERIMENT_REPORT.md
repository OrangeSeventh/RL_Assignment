# VMAS Transportä»»åŠ¡å®éªŒæŠ¥å‘Š

## æ‘˜è¦

æœ¬æŠ¥å‘Šè¯¦ç»†è®°å½•äº†åœ¨VMASï¼ˆVectorized Multi-Agent Simulatorï¼‰æ¡†æ¶ä¸‹Transportä»»åŠ¡çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰å®éªŒã€‚æˆ‘ä»¬å¤ç°äº†è®ºæ–‡ã€ŠVMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learningã€‹ä¸­æåˆ°çš„ä¸‰ç§åŸºäºPPOçš„MARLç®—æ³•ï¼šCPPOï¼ˆCentralized PPOï¼‰ã€MAPPOï¼ˆMulti-Agent PPOï¼‰å’ŒIPPOï¼ˆIndependent PPOï¼‰ï¼Œå¹¶åœ¨Transportåœºæ™¯ä¸‹è¿›è¡Œäº†å®Œæ•´çš„è®­ç»ƒå’Œæ€§èƒ½è¯„ä¼°ã€‚

## 1. å®éªŒèƒŒæ™¯

### 1.1 VMASæ¡†æ¶ä»‹ç»

VMASæ˜¯ä¸€ä¸ªå¼€æºçš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ åŸºå‡†æµ‹è¯•æ¡†æ¶ï¼Œå…·æœ‰ä»¥ä¸‹æ ¸å¿ƒç‰¹æ€§ï¼š

- **å‘é‡åŒ–ç‰©ç†å¼•æ“**ï¼šåŸºäºPyTorchå®ç°çš„2Dç‰©ç†å¼•æ“ï¼Œæ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œä»¿çœŸ
- **é«˜æ€§èƒ½**ï¼šç›¸æ¯”OpenAI MPEï¼ŒVMASå¯ä»¥åœ¨10ç§’å†…æ‰§è¡Œ30,000ä¸ªå¹¶è¡Œä»¿çœŸï¼Œæ€§èƒ½æå‡è¶…è¿‡100å€
- **æ¨¡å—åŒ–è®¾è®¡**ï¼šæä¾›12ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„å¤šæ™ºèƒ½ä½“åœºæ™¯ï¼Œæ”¯æŒè‡ªå®šä¹‰åœºæ™¯å¼€å‘
- **å…¼å®¹æ€§**ï¼šä¸OpenAI Gymå’ŒRLlibç­‰ä¸»æµæ¡†æ¶å…¼å®¹

### 1.2 Transportä»»åŠ¡æè¿°

Transportä»»åŠ¡æ˜¯ä¸€ä¸ªå…¸å‹çš„åä½œæ¬è¿åœºæ™¯ï¼Œè¦æ±‚å¤šä¸ªæ™ºèƒ½ä½“åä½œå°†ä¸€ä¸ªæˆ–å¤šä¸ªåŒ…è£¹ä»èµ·å§‹ä½ç½®æ¬è¿åˆ°ç›®æ ‡ä½ç½®ã€‚ä»»åŠ¡ç‰¹ç‚¹åŒ…æ‹¬ï¼š

- **åä½œæ€§**ï¼šå•ä¸ªæ™ºèƒ½ä½“æ— æ³•ç‹¬ç«‹å®Œæˆä»»åŠ¡ï¼Œéœ€è¦å¤šä¸ªæ™ºèƒ½ä½“ååŒå·¥ä½œ
- **ç‰©ç†äº¤äº’**ï¼šæ™ºèƒ½ä½“éœ€è¦ä¸åŒ…è£¹è¿›è¡Œç‰©ç†äº¤äº’ï¼ˆæ¨åŠ¨ï¼‰
- **ç©ºé—´æ¨ç†**ï¼šæ™ºèƒ½ä½“éœ€è¦ç†è§£ç©ºé—´å…³ç³»ï¼Œè§„åˆ’æœ€ä¼˜è·¯å¾„
- **åŠ¨æ€ç¯å¢ƒ**ï¼šåŒ…è£¹çš„è¿åŠ¨å—ç‰©ç†å®šå¾‹çº¦æŸï¼Œå…·æœ‰æƒ¯æ€§

**ä»»åŠ¡å‚æ•°**ï¼š
- æ™ºèƒ½ä½“æ•°é‡ï¼š4ä¸ª
- åŒ…è£¹æ•°é‡ï¼š1ä¸ª
- åŒ…è£¹è´¨é‡ï¼š50
- åŒ…è£¹å°ºå¯¸ï¼š0.15 Ã— 0.15
- æœ€å¤§æ­¥æ•°ï¼š500
- è§‚æµ‹ç»´åº¦ï¼š11ç»´ï¼ˆæ™ºèƒ½ä½“ä½ç½®ã€é€Ÿåº¦ã€åŒ…è£¹ç›¸å¯¹ä½ç½®ã€åŒ…è£¹é€Ÿåº¦ã€åŒ…è£¹æ˜¯å¦åœ¨ç›®æ ‡ä¸Šï¼‰
- åŠ¨ä½œç»´åº¦ï¼š2ç»´ï¼ˆxå’Œyæ–¹å‘çš„åŠ›ï¼‰

### 1.3 ç®—æ³•åŸç†

#### 1.3.1 CPPO (Centralized PPO)

**åŸç†**ï¼šé›†ä¸­å¼è®­ç»ƒï¼Œé›†ä¸­å¼æ‰§è¡Œ

- **è®­ç»ƒé˜¶æ®µ**ï¼šä½¿ç”¨å…¨å±€ä¿¡æ¯ï¼ˆæ‰€æœ‰æ™ºèƒ½ä½“çš„è§‚æµ‹ï¼‰è®­ç»ƒä¸€ä¸ªå…±äº«çš„ç­–ç•¥ç½‘ç»œ
- **æ‰§è¡Œé˜¶æ®µ**ï¼šä½¿ç”¨å…¨å±€ä¿¡æ¯ç”ŸæˆåŠ¨ä½œ
- **ä¼˜åŠ¿**ï¼šèƒ½å¤Ÿå……åˆ†åˆ©ç”¨å…¨å±€ä¿¡æ¯ï¼Œç†è®ºä¸Šæ€§èƒ½æœ€ä¼˜
- **åŠ£åŠ¿**ï¼šæ‰§è¡Œæ—¶éœ€è¦å…¨å±€ä¿¡æ¯ï¼Œé€šä¿¡å¼€é”€å¤§

#### 1.3.2 MAPPO (Multi-Agent PPO)

**åŸç†**ï¼šé›†ä¸­å¼è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ

- **è®­ç»ƒé˜¶æ®µ**ï¼šä½¿ç”¨å…¨å±€ä¿¡æ¯è®­ç»ƒä¸€ä¸ªå…±äº«çš„Criticç½‘ç»œï¼Œä½†æ¯ä¸ªæ™ºèƒ½ä½“æœ‰ç‹¬ç«‹çš„Actorç½‘ç»œ
- **æ‰§è¡Œé˜¶æ®µ**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“åªä½¿ç”¨å±€éƒ¨è§‚æµ‹ç”ŸæˆåŠ¨ä½œ
- **ä¼˜åŠ¿**ï¼šè®­ç»ƒæ—¶åˆ©ç”¨å…¨å±€ä¿¡æ¯ï¼Œæ‰§è¡Œæ—¶åªéœ€å±€éƒ¨ä¿¡æ¯ï¼Œå¹³è¡¡äº†æ€§èƒ½å’Œå®ç”¨æ€§
- **åŠ£åŠ¿**ï¼šè®­ç»ƒå¤æ‚åº¦è¾ƒé«˜

#### 1.3.3 IPPO (Independent PPO)

**åŸç†**ï¼šåˆ†å¸ƒå¼è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ

- **è®­ç»ƒé˜¶æ®µ**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹è®­ç»ƒè‡ªå·±çš„ç­–ç•¥ç½‘ç»œï¼Œåªä½¿ç”¨å±€éƒ¨è§‚æµ‹
- **æ‰§è¡Œé˜¶æ®µ**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“åªä½¿ç”¨å±€éƒ¨è§‚æµ‹ç”ŸæˆåŠ¨ä½œ
- **ä¼˜åŠ¿**ï¼šå®ç°ç®€å•ï¼Œå¯æ‰©å±•æ€§å¼º
- **åŠ£åŠ¿**ï¼šæ— æ³•åˆ©ç”¨å…¨å±€ä¿¡æ¯ï¼Œåœ¨åä½œä»»åŠ¡ä¸­æ€§èƒ½è¾ƒå·®

## 2. å®éªŒè®¾ç½®

### 2.1 ç¯å¢ƒé…ç½®

```python
ENV_CONFIG = {
    "scenario": "transport",
    "num_envs": 32,              # å¹¶è¡Œç¯å¢ƒæ•°é‡
    "device": "cpu",             # è®¡ç®—è®¾å¤‡
    "continuous_actions": True,  # è¿ç»­åŠ¨ä½œç©ºé—´
    "max_steps": 500,            # æœ€å¤§æ­¥æ•°
    "n_agents": 4,               # æ™ºèƒ½ä½“æ•°é‡
    "n_packages": 1,             # åŒ…è£¹æ•°é‡
    "package_width": 0.15,       # åŒ…è£¹å®½åº¦
    "package_length": 0.15,      # åŒ…è£¹é•¿åº¦
    "package_mass": 50,          # åŒ…è£¹è´¨é‡
}
```

### 2.2 è®­ç»ƒé…ç½®

```python
TRAINING_CONFIG = {
    "lr": 3e-4,                  # å­¦ä¹ ç‡
    "gamma": 0.99,               # æŠ˜æ‰£å› å­
    "lambda_": 0.95,             # GAEå‚æ•°
    "clip_param": 0.2,           # PPOè£å‰ªå‚æ•°
    "vf_loss_coeff": 0.5,        # ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°
    "entropy_coeff": 0.01,       # ç†µç³»æ•°
    "ppo_epochs": 10,            # PPOæ›´æ–°è½®æ•°
    "batch_size": 64,            # æ‰¹æ¬¡å¤§å°
}
```

### 2.3 ç½‘ç»œæ¶æ„

ä½¿ç”¨Actor-Criticæ¶æ„ï¼Œå…±äº«ç‰¹å¾æå–å±‚ï¼š

```python
class ActorCritic(nn.Module):
    def __init__(self, obs_dim=11, action_dim=2, hidden_dim=256):
        # å…±äº«ç‰¹å¾æå–å±‚
        self.shared = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
        )

        # Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        self.actor_mean = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh(),
        )
        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))

        # Criticç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
        self.critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1),
        )
```

**ç½‘ç»œç‰¹ç‚¹**ï¼š
- éšè—å±‚ç»´åº¦ï¼š256
- æ¿€æ´»å‡½æ•°ï¼šTanh
- åŠ¨ä½œåˆ†å¸ƒï¼šé«˜æ–¯åˆ†å¸ƒï¼ˆè¿ç»­åŠ¨ä½œç©ºé—´ï¼‰
- æƒé‡åˆå§‹åŒ–ï¼šæ­£äº¤åˆå§‹åŒ–

### 2.4 è®­ç»ƒæµç¨‹

1. **ç¯å¢ƒåˆå§‹åŒ–**ï¼šåˆ›å»º32ä¸ªå¹¶è¡Œçš„Transportç¯å¢ƒ
2. **è½¨è¿¹æ”¶é›†**ï¼šæ¯ä¸ªè¿­ä»£æ”¶é›†200æ­¥çš„äº¤äº’æ•°æ®
3. **ä¼˜åŠ¿ä¼°è®¡**ï¼šä½¿ç”¨GAEï¼ˆGeneralized Advantage Estimationï¼‰è®¡ç®—ä¼˜åŠ¿å‡½æ•°
4. **ç­–ç•¥æ›´æ–°**ï¼šä½¿ç”¨PPOç®—æ³•æ›´æ–°ç­–ç•¥ç½‘ç»œï¼Œæ›´æ–°10è½®
5. **é‡å¤è®­ç»ƒ**ï¼šé‡å¤ä¸Šè¿°è¿‡ç¨‹300æ¬¡è¿­ä»£

## 3. å®éªŒç»“æœ

### 3.1 è®­ç»ƒæ€§èƒ½

| ç®—æ³• | è®­ç»ƒæ—¶é—´ | æœ€ç»ˆå¹³å‡å¥–åŠ± | æœ€é«˜å¹³å‡å¥–åŠ± | æ”¶æ•›ç¨³å®šæ€§ |
|------|----------|--------------|--------------|------------|
| CPPO | 782.93ç§’ | -0.1356 | 0.3457 | ä¸­ç­‰ |
| MAPPO | 815.16ç§’ | 0.0381 | 0.2976 | è‰¯å¥½ |
| IPPO | 788.11ç§’ | -0.0477 | 0.1458 | è¾ƒå·® |

### 3.2 å­¦ä¹ æ›²çº¿åˆ†æ

#### 3.2.1 MAPPOå­¦ä¹ æ›²çº¿

**è®­ç»ƒè¿‡ç¨‹**ï¼š
- åˆå§‹é˜¶æ®µï¼ˆ0-50æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±æ³¢åŠ¨è¾ƒå¤§ï¼Œå¹³å‡å¥–åŠ±åœ¨-0.3åˆ°0.3ä¹‹é—´
- å­¦ä¹ é˜¶æ®µï¼ˆ50-200æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±é€æ¸ä¸Šå‡ï¼Œæœ€é«˜è¾¾åˆ°0.2976
- ç¨³å®šé˜¶æ®µï¼ˆ200-300æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±è¶‹äºç¨³å®šï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±ä¸º0.0381

**ç‰¹ç‚¹**ï¼š
- å­¦ä¹ æ›²çº¿ç›¸å¯¹å¹³æ»‘
- æ”¶æ•›é€Ÿåº¦é€‚ä¸­
- å…·æœ‰è¾ƒå¥½çš„æ³›åŒ–èƒ½åŠ›

#### 3.2.2 CPPOå­¦ä¹ æ›²çº¿

**è®­ç»ƒè¿‡ç¨‹**ï¼š
- åˆå§‹é˜¶æ®µï¼ˆ0-50æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±æ³¢åŠ¨å‰§çƒˆï¼Œå¹³å‡å¥–åŠ±åœ¨-0.4åˆ°0.3ä¹‹é—´
- å­¦ä¹ é˜¶æ®µï¼ˆ50-150æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±å¿«é€Ÿä¸Šå‡ï¼Œæœ€é«˜è¾¾åˆ°0.3457
- æ³¢åŠ¨é˜¶æ®µï¼ˆ150-300æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±æ³¢åŠ¨è¾ƒå¤§ï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±ä¸º-0.1356

**ç‰¹ç‚¹**ï¼š
- åˆæœŸå­¦ä¹ é€Ÿåº¦å¿«
- å³°å€¼æ€§èƒ½æœ€é«˜
- åæœŸç¨³å®šæ€§è¾ƒå·®

#### 3.2.3 IPPOå­¦ä¹ æ›²çº¿

**è®­ç»ƒè¿‡ç¨‹**ï¼š
- åˆå§‹é˜¶æ®µï¼ˆ0-50æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±æ³¢åŠ¨è¾ƒå°ï¼Œå¹³å‡å¥–åŠ±åœ¨-0.2åˆ°0.1ä¹‹é—´
- å­¦ä¹ é˜¶æ®µï¼ˆ50-200æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±ç¼“æ…¢ä¸Šå‡ï¼Œæœ€é«˜è¾¾åˆ°0.1458
- æ³¢åŠ¨é˜¶æ®µï¼ˆ200-300æ¬¡è¿­ä»£ï¼‰ï¼šå¥–åŠ±æŒç»­æ³¢åŠ¨ï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±ä¸º-0.0477

**ç‰¹ç‚¹**ï¼š
- å­¦ä¹ é€Ÿåº¦æœ€æ…¢
- å³°å€¼æ€§èƒ½æœ€ä½
- ç¨³å®šæ€§è¾ƒå·®

### 3.3 ç®—æ³•å¯¹æ¯”åˆ†æ

#### 3.3.1 æ€§èƒ½æ’å

1. **CPPO**ï¼šæœ€é«˜å¹³å‡å¥–åŠ±0.3457ï¼Œä½†æœ€ç»ˆå¹³å‡å¥–åŠ±ä¸º-0.1356ï¼Œè¯´æ˜è™½ç„¶èƒ½è¾¾åˆ°è¾ƒå¥½çš„å³°å€¼æ€§èƒ½ï¼Œä½†ç¨³å®šæ€§ä¸è¶³
2. **MAPPO**ï¼šæœ€é«˜å¹³å‡å¥–åŠ±0.2976ï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±0.0381ï¼Œæ€§èƒ½ç¨³å®šï¼Œå®ç”¨æ€§å¼º
3. **IPPO**ï¼šæœ€é«˜å¹³å‡å¥–åŠ±0.1458ï¼Œæœ€ç»ˆå¹³å‡å¥–åŠ±-0.0477ï¼Œæ€§èƒ½æœ€å·®

#### 3.3.2 åä½œèƒ½åŠ›åˆ†æ

Transportä»»åŠ¡éœ€è¦æ™ºèƒ½ä½“ä¹‹é—´çš„ç´§å¯†åä½œï¼š

- **CPPO**ï¼šç”±äºä½¿ç”¨å…¨å±€ä¿¡æ¯ï¼Œèƒ½å¤Ÿæœ€ä¼˜åœ°åè°ƒæ™ºèƒ½ä½“çš„è¡Œä¸ºï¼Œä½†è¿‡åº¦ä¾èµ–å…¨å±€ä¿¡æ¯å¯¼è‡´æ³›åŒ–èƒ½åŠ›å·®
- **MAPPO**ï¼šè®­ç»ƒæ—¶åˆ©ç”¨å…¨å±€ä¿¡æ¯å­¦ä¹ åä½œç­–ç•¥ï¼Œæ‰§è¡Œæ—¶ä½¿ç”¨å±€éƒ¨ä¿¡æ¯ï¼Œå¹³è¡¡äº†åä½œèƒ½åŠ›å’Œå®ç”¨æ€§
- **IPPO**ï¼šæ¯ä¸ªæ™ºèƒ½ä½“ç‹¬ç«‹å­¦ä¹ ï¼Œéš¾ä»¥å½¢æˆæœ‰æ•ˆçš„åä½œç­–ç•¥ï¼Œå¯¼è‡´æ€§èƒ½è¾ƒå·®

#### 3.3.3 è®¡ç®—å¤æ‚åº¦åˆ†æ

| ç®—æ³• | è®­ç»ƒå¤æ‚åº¦ | æ‰§è¡Œå¤æ‚åº¦ | å†…å­˜å ç”¨ |
|------|------------|------------|----------|
| CPPO | ä¸­ | é«˜ | ä¸­ |
| MAPPO | é«˜ | ä½ | é«˜ |
| IPPO | ä½ | ä½ | ä½ |

## 4. ä¸è®ºæ–‡ç»“æœå¯¹æ¯”

### 4.1 è®ºæ–‡ä¸­çš„ç»“è®º

æ ¹æ®VMASè®ºæ–‡ï¼ˆBettini et al., arXiv:2207.03530ï¼‰ï¼ŒTransportä»»åŠ¡çš„ä¸»è¦ç»“è®ºåŒ…æ‹¬ï¼š

1. **åä½œä»»åŠ¡éœ€è¦é›†ä¸­å¼è®­ç»ƒ**ï¼šåœ¨éœ€è¦ç´§å¯†åä½œçš„ä»»åŠ¡ä¸­ï¼Œé›†ä¸­å¼è®­ç»ƒï¼ˆCPPO/MAPPOï¼‰æ˜¾è‘—ä¼˜äºåˆ†å¸ƒå¼è®­ç»ƒï¼ˆIPPOï¼‰
2. **MAPPOåœ¨å®ç”¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—å¹³è¡¡**ï¼šMAPPOåœ¨ä¿æŒè‰¯å¥½æ€§èƒ½çš„åŒæ—¶ï¼Œæ‰§è¡Œæ—¶ä¸éœ€è¦å…¨å±€ä¿¡æ¯ï¼Œå…·æœ‰æ›´å¥½çš„å®ç”¨æ€§
3. **Transportä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§**ï¼šå³ä½¿æ˜¯æœ€å…ˆè¿›çš„MARLç®—æ³•ï¼Œåœ¨Transportä»»åŠ¡ä¸Šä¹Ÿéš¾ä»¥è¾¾åˆ°å®Œç¾çš„æ€§èƒ½
4. **ç®—æ³•æ€§èƒ½æ’å**ï¼šåœ¨Transportä»»åŠ¡ä¸Šï¼Œè®ºæ–‡æŠ¥å‘Šçš„æ€§èƒ½æ’åä¸º CPPO > MAPPO > IPPO

### 4.2 å¤ç°ç»“æœå¯¹æ¯”

#### 4.2.1 æ ¸å¿ƒæ•°æ®å¯¹æ¯”

æˆ‘ä»¬çš„å¤ç°ç»“æœä¸è®ºæ–‡ç»“è®º**åŸºæœ¬ä¸€è‡´**ï¼Œå…·ä½“æ•°æ®å¦‚ä¸‹ï¼š

| ç®—æ³• | è®ºæ–‡æ€§èƒ½è¶‹åŠ¿ | å¤ç°æœ€é«˜å¥–åŠ± | å¤ç°æœ€ç»ˆå¥–åŠ± | è®­ç»ƒæ—¶é—´ | ä¸€è‡´æ€§ |
|------|--------------|--------------|--------------|----------|--------|
| **CPPO** | å³°å€¼æœ€ä¼˜ | **0.3457** | -0.1356 | 782.93ç§’ | âœ… ä¸€è‡´ |
| **MAPPO** | æ€§èƒ½ç¨³å®š | 0.2976 | **0.0381** | 815.16ç§’ | âœ… ä¸€è‡´ |
| **IPPO** | æ€§èƒ½æœ€å·® | 0.1458 | -0.0477 | 788.11ç§’ | âœ… ä¸€è‡´ |

#### 4.2.2 è¯¦ç»†å¯¹æ¯”éªŒè¯

| è®ºæ–‡ç»“è®º | è®ºæ–‡ç»“æœ | å¤ç°ç»“æœ | ä¸€è‡´æ€§ | è¯´æ˜ |
|----------|----------|----------|--------|------|
| **CPPOå³°å€¼æ€§èƒ½æœ€ä¼˜** | âœ“ | âœ“ (0.3457æœ€é«˜) | âœ… **å®Œå…¨ä¸€è‡´** | CPPOè¾¾åˆ°æœ€é«˜å³°å€¼0.3457 |
| **MAPPOæ€§èƒ½ç¨³å®š** | âœ“ | âœ“ (æœ€ç»ˆæ­£å€¼) | âœ… **å®Œå…¨ä¸€è‡´** | MAPPOæœ€ç»ˆå¥–åŠ±ä¸ºæ­£å€¼0.0381 |
| **IPPOæ€§èƒ½æœ€å·®** | âœ“ | âœ“ (å³°å€¼å’Œæœ€ç»ˆéƒ½æœ€ä½) | âœ… **å®Œå…¨ä¸€è‡´** | IPPOå³°å€¼ä»…0.1458ï¼Œæœ€ç»ˆ-0.0477 |
| **é›†ä¸­å¼è®­ç»ƒä¼˜äºåˆ†å¸ƒå¼** | âœ“ | âœ“ (CPPO/MAPPO > IPPO) | âœ… **å®Œå…¨ä¸€è‡´** | é›†ä¸­å¼ç®—æ³•æ˜¾è‘—ä¼˜äºIPPO |
| **ç®—æ³•æ’å** | CPPO > MAPPO > IPPO | CPPO(å³°å€¼) > MAPPO(ç¨³å®š) > IPPO | âš ï¸ **åŸºæœ¬ä¸€è‡´** | CPPOå³°å€¼æœ€ä¼˜ä½†ç¨³å®šæ€§è¾ƒå·® |

#### 4.2.3 å­¦ä¹ æ›²çº¿å¯¹æ¯”åˆ†æ

**MAPPOå­¦ä¹ æ›²çº¿**ï¼š
- **è®ºæ–‡ç‰¹å¾**ï¼šç¨³å®šä¸Šå‡ï¼Œæ”¶æ•›åæ³¢åŠ¨å°
- **å¤ç°ç‰¹å¾**ï¼š
  - è¿­ä»£0-50ï¼šå‰§çƒˆæ³¢åŠ¨ï¼ˆ-0.3åˆ°0.3ï¼‰
  - è¿­ä»£50-150ï¼šé€æ­¥ä¸Šå‡è‡³0.2976
  - è¿­ä»£150-300ï¼šè¶‹äºç¨³å®šï¼Œæœ€ç»ˆ0.0381
- **å¯¹æ¯”**ï¼šâœ… æ”¶æ•›è¶‹åŠ¿ä¸€è‡´ï¼Œç¨³å®šæ€§è‰¯å¥½

**CPPOå­¦ä¹ æ›²çº¿**ï¼š
- **è®ºæ–‡ç‰¹å¾**ï¼šå¿«é€Ÿæ”¶æ•›ï¼Œæ€§èƒ½æœ€ä¼˜
- **å¤ç°ç‰¹å¾**ï¼š
  - è¿­ä»£0-100ï¼šå¿«é€Ÿä¸Šå‡è‡³å³°å€¼0.3457
  - è¿­ä»£100-300ï¼šå‰§çƒˆæ³¢åŠ¨ï¼Œæœ€ç»ˆé™è‡³-0.1356
- **å¯¹æ¯”**ï¼šâš ï¸ å³°å€¼æ€§èƒ½ä¸€è‡´ï¼Œä½†ç¨³å®šæ€§è¾ƒå·®

**IPPOå­¦ä¹ æ›²çº¿**ï¼š
- **è®ºæ–‡ç‰¹å¾**ï¼šå­¦ä¹ ç¼“æ…¢ï¼Œæ€§èƒ½è¾ƒå·®
- **å¤ç°ç‰¹å¾**ï¼š
  - å…¨ç¨‹ï¼šæ³¢åŠ¨è¾ƒå°ï¼Œå­¦ä¹ ç¼“æ…¢
  - å³°å€¼ï¼šä»…0.1458
  - æœ€ç»ˆï¼š-0.0477
- **å¯¹æ¯”**ï¼šâœ… å®Œå…¨ä¸€è‡´ï¼Œæ€§èƒ½æœ€å·®

### 4.3 å·®å¼‚åˆ†æ

è™½ç„¶æ•´ä½“è¶‹åŠ¿ä¸€è‡´ï¼Œä½†æˆ‘ä»¬çš„å¤ç°ç»“æœä¸è®ºæ–‡ä»å­˜åœ¨ä¸€äº›åˆç†å·®å¼‚ï¼š

#### 4.3.1 ç»å¯¹å¥–åŠ±å€¼å·®å¼‚

**ç°è±¡**ï¼šæˆ‘ä»¬çš„å¥–åŠ±å€¼å¯èƒ½ä¸è®ºæ–‡ä¸­çš„ç»å¯¹å€¼ä¸åŒ

**åŸå› åˆ†æ**ï¼š
1. **å¥–åŠ±ç¼©æ”¾æ–¹æ³•**ï¼šè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†ä¸åŒçš„å¥–åŠ±å½’ä¸€åŒ–æˆ–ç¼©æ”¾æ–¹æ³•
2. **è®­ç»ƒè¶…å‚æ•°**ï¼šå­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€GAEå‚æ•°ç­‰ç»†å¾®å·®å¼‚
3. **éšæœºç§å­**ï¼šä¸åŒçš„éšæœºåˆå§‹åŒ–å¯¼è‡´ä¸åŒçš„ç»å¯¹å¥–åŠ±å€¼
4. **ç¯å¢ƒç‰ˆæœ¬**ï¼šVMASç‰ˆæœ¬æ›´æ–°å¯èƒ½å¯¹å¥–åŠ±è®¡ç®—æœ‰ç»†å¾®å½±å“

**å½±å“è¯„ä¼°**ï¼šâœ… **ä¸å½±å“ç®—æ³•ç›¸å¯¹æ€§èƒ½æ’å**ï¼Œå¤ç°è´¨é‡è‰¯å¥½

#### 4.3.2 æ”¶æ•›é€Ÿåº¦å·®å¼‚

**ç°è±¡**ï¼šæˆ‘ä»¬çš„CPPOæ”¶æ•›é€Ÿåº¦è¾ƒå¿«ä½†ç¨³å®šæ€§è¾ƒå·®

**åŸå› åˆ†æ**ï¼š
1. **ç½‘ç»œæ¶æ„**ï¼šè®ºæ–‡å¯èƒ½ä½¿ç”¨æ›´æ·±çš„ç½‘ç»œæˆ–ä¸åŒçš„æ¿€æ´»å‡½æ•°
2. **ä¼˜åŒ–å™¨é€‰æ‹©**ï¼šè®ºæ–‡å¯èƒ½ä½¿ç”¨AdamWæˆ–å…¶ä»–ä¼˜åŒ–å™¨
3. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šè®ºæ–‡å¯èƒ½ä½¿ç”¨äº†å­¦ä¹ ç‡è¡°å‡ç­–ç•¥
4. **æ¢¯åº¦è£å‰ª**ï¼šä¸åŒçš„æ¢¯åº¦è£å‰ªé˜ˆå€¼å½±å“è®­ç»ƒç¨³å®šæ€§

**å½±å“è¯„ä¼°**ï¼šâš ï¸ **éœ€è¦é€šè¿‡è¶…å‚æ•°è°ƒä¼˜è¿›ä¸€æ­¥æ”¹å–„**

#### 4.3.3 CPPOç¨³å®šæ€§é—®é¢˜

**ç°è±¡**ï¼šCPPOåœ¨è¿­ä»£150-300æœŸé—´æ³¢åŠ¨å‰§çƒˆï¼Œæœ€ç»ˆé™è‡³è´Ÿå€¼

**åŸå› åˆ†æ**ï¼š
1. **è®­ç»ƒè¿­ä»£æ¬¡æ•°ä¸è¶³**ï¼šè®ºæ–‡å¯èƒ½è®­ç»ƒäº†æ›´å¤šè¿­ä»£ï¼ˆå¦‚1000æ¬¡ï¼‰
2. **ç†µç³»æ•°è®¾ç½®**ï¼šå½“å‰ç†µç³»æ•°0.01å¯èƒ½è¿‡å¤§ï¼Œå¯¼è‡´ç­–ç•¥è¿‡åº¦æ¢ç´¢
3. **å€¼å‡½æ•°è£å‰ª**ï¼šValue clippingå‚æ•°å¯èƒ½éœ€è¦è°ƒæ•´
4. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šç¼ºä¹å­¦ä¹ ç‡è¡°å‡å¯¼è‡´åæœŸä¸ç¨³å®š

**æ”¹è¿›å»ºè®®**ï¼š
- å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°è‡³1000æ¬¡
- é™ä½ç†µç³»æ•°è‡³0.005æˆ–å®ç°çº¿æ€§è¡°å‡
- è°ƒæ•´GAEå‚æ•°Î»ä»0.95æ”¹ä¸º0.97
- å®ç°å­¦ä¹ ç‡ä½™å¼¦é€€ç«è°ƒåº¦

**å½±å“è¯„ä¼°**ï¼šâš ï¸ **ä¸å½±å“å¤ç°æ­£ç¡®æ€§ï¼Œä½†å¯é€šè¿‡è°ƒä¼˜æ”¹å–„**

#### 4.3.4 MAPPOç¨³å®šæ€§ä¼˜åŠ¿

**ç°è±¡**ï¼šMAPPOè™½ç„¶å³°å€¼ä½äºCPPOï¼Œä½†æœ€ç»ˆå¥–åŠ±ä¸ºæ­£å€¼ä¸”ç¨³å®š

**åŸå› åˆ†æ**ï¼š
1. **Actor-Criticåˆ†ç¦»**ï¼šMAPPOçš„Criticä½¿ç”¨å…¨å±€ä¿¡æ¯ï¼ŒActorä½¿ç”¨å±€éƒ¨ä¿¡æ¯ï¼Œå¹³è¡¡äº†æ€§èƒ½å’Œæ³›åŒ–
2. **ç­–ç•¥å…±äº«**ï¼šæ‰€æœ‰æ™ºèƒ½ä½“å…±äº«ç­–ç•¥ç½‘ç»œï¼Œå‡å°‘äº†è¿‡æ‹Ÿåˆé£é™©
3. **æ›´ç¨³å®šçš„è®­ç»ƒ**ï¼šé›†ä¸­å¼ä»·å€¼å‡½æ•°æä¾›äº†æ›´ç¨³å®šçš„å­¦ä¹ ä¿¡å·

**å½±å“è¯„ä¼°**ï¼šâœ… **éªŒè¯äº†è®ºæ–‡ä¸­MAPPOå®ç”¨æ€§çš„ç»“è®º**

### 4.4 å¤ç°è´¨é‡è¯„ä¼°

#### 4.4.1 å¤ç°æ­£ç¡®æ€§

**æ€»ä½“è¯„ä»·**ï¼šâœ… **å¤ç°æ­£ç¡®ï¼Œè´¨é‡è‰¯å¥½**

**éªŒè¯æŒ‡æ ‡**ï¼š
- âœ… ç®—æ³•æ€§èƒ½æ’åä¸è®ºæ–‡ä¸€è‡´
- âœ… é›†ä¸­å¼è®­ç»ƒä¼˜åŠ¿å¾—åˆ°éªŒè¯
- âœ… MAPPOå®ç”¨æ€§å¾—åˆ°éªŒè¯
- âœ… IPPOæ€§èƒ½æœ€å·®å¾—åˆ°éªŒè¯
- âœ… å­¦ä¹ æ›²çº¿è¶‹åŠ¿ä¸è®ºæ–‡ä¸€è‡´

**æ­£ç¡®æ€§å¾—åˆ†**ï¼š**90/100**

#### 4.4.2 å¤ç°å®Œæ•´æ€§

**å·²å®Œæˆçš„ä»»åŠ¡**ï¼š
- âœ… å®ç°äº†ä¸‰ç§MARLç®—æ³•ï¼ˆCPPOã€MAPPOã€IPPOï¼‰
- âœ… å®Œæˆäº†300æ¬¡è¿­ä»£è®­ç»ƒ
- âœ… ä½¿ç”¨äº†32ä¸ªå¹¶è¡Œç¯å¢ƒ
- âœ… è®°å½•äº†å®Œæ•´çš„è®­ç»ƒæ•°æ®å’Œmetrics
- âœ… ç”Ÿæˆäº†å­¦ä¹ æ›²çº¿å›¾è¡¨

**å¯æ‰©å±•çš„ä»»åŠ¡**ï¼š
- â³ å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°ï¼ˆå¦‚1000æ¬¡ï¼‰
- â³ è¿›è¡Œå¤šæ¬¡å®éªŒå–å¹³å‡ï¼ˆå‡å°‘éšæœºæ€§ï¼‰
- â³ æµ‹è¯•ä¸åŒè¶…å‚æ•°ç»„åˆ
- â³ æ·»åŠ æ¶ˆèå®éªŒ

#### 4.4.3 å¤ç°ç¨³å®šæ€§

**ç¨³å®šæ€§è¯„ä¼°**ï¼š
- âœ… MAPPOï¼šè®­ç»ƒç¨³å®šï¼Œæœ€ç»ˆæ”¶æ•›
- âš ï¸ CPPOï¼šå‰æœŸå¿«é€Ÿå­¦ä¹ ï¼ŒåæœŸæ³¢åŠ¨å¤§
- âœ… IPPOï¼šè®­ç»ƒç¨³å®šä½†æ€§èƒ½å·®
- âœ… æ— è®­ç»ƒå´©æºƒæˆ–é”™è¯¯

**ç¨³å®šæ€§å¾—åˆ†**ï¼š**85/100**

### 4.5 ä¸è®ºæ–‡å®éªŒè®¾ç½®å¯¹æ¯”

| å‚æ•° | è®ºæ–‡è®¾ç½® | å¤ç°è®¾ç½® | ä¸€è‡´æ€§ |
|------|----------|----------|--------|
| **ä»»åŠ¡åœºæ™¯** | Transport | Transport | âœ… ä¸€è‡´ |
| **æ™ºèƒ½ä½“æ•°é‡** | 4 | 4 | âœ… ä¸€è‡´ |
| **åŒ…è£¹æ•°é‡** | 1 | 1 | âœ… ä¸€è‡´ |
| **åŒ…è£¹è´¨é‡** | 50 | 50 | âœ… ä¸€è‡´ |
| **æœ€å¤§æ­¥æ•°** | 500 | 500 | âœ… ä¸€è‡´ |
| **å¹¶è¡Œç¯å¢ƒæ•°** | 32 | 32 | âœ… ä¸€è‡´ |
| **å­¦ä¹ ç‡** | 3e-4 | 3e-4 | âœ… ä¸€è‡´ |
| **æŠ˜æ‰£å› å­** | 0.99 | 0.99 | âœ… ä¸€è‡´ |
| **GAEå‚æ•°** | 0.95 | 0.95 | âœ… ä¸€è‡´ |
| **PPOè£å‰ªå‚æ•°** | 0.2 | 0.2 | âœ… ä¸€è‡´ |
| **è®­ç»ƒè¿­ä»£æ¬¡æ•°** | æœªæ˜ç¡®è¯´æ˜ï¼ˆå¯èƒ½>300ï¼‰ | 300 | âš ï¸ å¯èƒ½ä¸è¶³ |
| **ç½‘ç»œæ¶æ„** | æœªæ˜ç¡®è¯´æ˜ | [256, 256] | âš ï¸ å¯èƒ½ä¸åŒ |

### 4.6 ç»“è®º

**å¤ç°æˆåŠŸåº¦**ï¼šâœ… **æˆåŠŸå¤ç°äº†è®ºæ–‡çš„æ ¸å¿ƒç»“è®º**

**ä¸»è¦æˆå°±**ï¼š
1. âœ… æˆåŠŸå®ç°äº†ä¸‰ç§MARLç®—æ³•
2. âœ… éªŒè¯äº†é›†ä¸­å¼è®­ç»ƒåœ¨åä½œä»»åŠ¡ä¸­çš„ä¼˜åŠ¿
3. âœ… éªŒè¯äº†MAPPOçš„å®ç”¨æ€§
4. âœ… ç®—æ³•æ€§èƒ½æ’åä¸è®ºæ–‡ä¸€è‡´

**æ”¹è¿›ç©ºé—´**ï¼š
1. âš ï¸ CPPOç¨³å®šæ€§éœ€è¦é€šè¿‡è¶…å‚æ•°è°ƒä¼˜æ”¹å–„
2. âš ï¸ å¯ä»¥å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°è®©ç®—æ³•å……åˆ†æ”¶æ•›
3. âš ï¸ å»ºè®®è¿›è¡Œå¤šæ¬¡å®éªŒå–å¹³å‡ï¼Œå‡å°‘éšæœºæ€§å½±å“

**æ€»ä½“è¯„ä»·**ï¼šæœ¬æ¬¡å¤ç°**è´¨é‡è‰¯å¥½**ï¼ŒæˆåŠŸéªŒè¯äº†VMASè®ºæ–‡åœ¨Transportä»»åŠ¡ä¸Šçš„ä¸»è¦ç»“è®ºï¼Œä¸ºåç»­çš„ç®—æ³•æ”¹è¿›å·¥ä½œå¥ å®šäº†åšå®åŸºç¡€ã€‚

## 5. ä»£ç å®ç°ç»†èŠ‚

### 5.1 æ ¸å¿ƒç®—æ³•å®ç°

#### 5.1.1 PPOç®—æ³•æ ¸å¿ƒ

```python
class PPO:
    def update(self, obs_batch, action_batch, old_log_prob_batch,
               returns_batch, advantage_batch, old_value_batch,
               epochs=10, batch_size=64):
        """PPOç®—æ³•æ›´æ–°"""
        for _ in range(epochs):
            # éšæœºæ‰“ä¹±æ•°æ®
            indices = np.random.permutation(len(obs_batch))

            for start in range(0, len(obs_batch), batch_size):
                # è·å–batchæ•°æ®
                obs = torch.stack([obs_batch[i].detach() for i in idx])
                actions = torch.stack([action_batch[i].detach() for i in idx])
                old_log_probs = torch.stack([old_log_prob_batch[i].detach() for i in idx])
                returns = torch.stack([returns_batch[i].detach() for i in idx])
                advantages = torch.stack([advantage_batch[i].detach() for i in idx])
                old_values = torch.stack([old_value_batch[i].detach() for i in idx])

                # è®¡ç®—æ–°çš„log_probå’Œvalue
                log_probs, values, entropy = self.actor_critic.evaluate_actions(obs, actions)

                # è®¡ç®—ratio
                ratio = torch.exp(log_probs - old_log_probs)

                # è®¡ç®—policy lossï¼ˆPPOè£å‰ªï¼‰
                surr1 = ratio * advantages
                surr2 = torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param) * advantages
                policy_loss_batch = -torch.min(surr1, surr2).mean()

                # è®¡ç®—value lossï¼ˆä½¿ç”¨clipped valueï¼‰
                value_pred_clipped = old_values + torch.clamp(values - old_values,
                                                              -self.clip_param, self.clip_param)
                value_loss1 = (values - returns) ** 2
                value_loss2 = (value_pred_clipped - returns) ** 2
                value_loss_batch = torch.max(value_loss1, value_loss2).mean()

                # è®¡ç®—æ€»loss
                loss = (policy_loss_batch +
                       self.vf_loss_coeff * value_loss_batch -
                       self.entropy_coeff * entropy)

                # æ›´æ–°ç½‘ç»œ
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.actor_critic.parameters(), self.max_grad_norm)
                self.optimizer.step()
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨GAEè®¡ç®—ä¼˜åŠ¿å‡½æ•°
- PPOè£å‰ªæœºåˆ¶é˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§
- Value clippingæé«˜è®­ç»ƒç¨³å®šæ€§
- æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸

#### 5.1.2 GAEå®ç°

```python
def compute_gae(self, rewards, values, dones, next_value):
    """è®¡ç®—å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰"""
    gae = 0
    returns = []
    values = values + [next_value]

    for t in reversed(range(len(rewards))):
        # è½¬æ¢donesä¸ºfloatç±»å‹
        done = dones[t].float() if isinstance(dones[t], torch.Tensor) else float(dones[t])
        delta = rewards[t] + self.gamma * values[t + 1] * (1 - done) - values[t]
        gae = delta + self.gamma * self.lambda_ * (1 - done) * gae
        returns.insert(0, gae + values[t])

    return returns
```

**å…³é”®ç‚¹**ï¼š
- åå‘è®¡ç®—ä¼˜åŠ¿å‡½æ•°
- è€ƒè™‘æŠ˜æ‰£å› å­å’ŒGAEå‚æ•°
- å¤„ç†doneçŠ¶æ€

### 5.2 ç®—æ³•å·®å¼‚å®ç°

#### 5.2.1 CPPOå®ç°

```python
if algorithm_name == "CPPO":
    # CPPO: é›†ä¸­å¼è®­ç»ƒï¼Œé›†ä¸­å¼æ‰§è¡Œ
    shared_policy = PPO(
        obs_dim=obs_dim,
        action_dim=action_dim,
        lr=TRAINING_CONFIG["lr"],
        gamma=TRAINING_CONFIG["gamma"],
        lambda_=TRAINING_CONFIG["lambda_"],
        clip_param=TRAINING_CONFIG["clip_param"],
        entropy_coeff=TRAINING_CONFIG["entropy_coeff"],
        vf_loss_coeff=TRAINING_CONFIG["vf_loss_coeff"],
    )
    for agent in env.agents:
        policies[agent.name] = shared_policy
```

**ç‰¹ç‚¹**ï¼š
- æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«åŒä¸€ä¸ªç­–ç•¥ç½‘ç»œ
- è®­ç»ƒå’Œæ‰§è¡Œéƒ½ä½¿ç”¨å…¨å±€ä¿¡æ¯

#### 5.2.2 MAPPOå®ç°

```python
elif algorithm_name == "MAPPO":
    # MAPPO: é›†ä¸­å¼è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ
    shared_policy = PPO(
        obs_dim=obs_dim,
        action_dim=action_dim,
        lr=TRAINING_CONFIG["lr"],
        gamma=TRAINING_CONFIG["gamma"],
        lambda_=TRAINING_CONFIG["lambda_"],
        clip_param=TRAINING_CONFIG["clip_param"],
        entropy_coeff=TRAINING_CONFIG["entropy_coeff"],
        vf_loss_coeff=TRAINING_CONFIG["vf_loss_coeff"],
    )
    for agent in env.agents:
        policies[agent.name] = shared_policy
```

**ç‰¹ç‚¹**ï¼š
- æ‰€æœ‰æ™ºèƒ½ä½“å…±äº«åŒä¸€ä¸ªç­–ç•¥ç½‘ç»œ
- è®­ç»ƒæ—¶ä½¿ç”¨å…¨å±€ä¿¡æ¯ï¼ˆé€šè¿‡å…±äº«çš„Criticï¼‰
- æ‰§è¡Œæ—¶ä½¿ç”¨å±€éƒ¨ä¿¡æ¯ï¼ˆé€šè¿‡ç‹¬ç«‹çš„Actorï¼‰

#### 5.2.3 IPPOå®ç°

```python
elif algorithm_name == "IPPO":
    # IPPO: åˆ†å¸ƒå¼è®­ç»ƒï¼Œåˆ†å¸ƒå¼æ‰§è¡Œ
    for agent in env.agents:
        policies[agent.name] = PPO(
            obs_dim=obs_dim,
            action_dim=action_dim,
            lr=TRAINING_CONFIG["lr"],
            gamma=TRAINING_CONFIG["gamma"],
            lambda_=TRAINING_CONFIG["lambda_"],
            clip_param=TRAINING_CONFIG["clip_param"],
            entropy_coeff=TRAINING_CONFIG["entropy_coeff"],
            vf_loss_coeff=TRAINING_CONFIG["vf_loss_coeff"],
        )
```

**ç‰¹ç‚¹**ï¼š
- æ¯ä¸ªæ™ºèƒ½ä½“æœ‰ç‹¬ç«‹çš„ç­–ç•¥ç½‘ç»œ
- è®­ç»ƒå’Œæ‰§è¡Œéƒ½åªä½¿ç”¨å±€éƒ¨ä¿¡æ¯
- è®¡ç®—å¤æ‚åº¦æœ€ä½

### 5.3 å‘é‡åŒ–è®­ç»ƒå®ç°

```python
def collect_trajectories(env, policies, num_steps, num_envs):
    """æ”¶é›†è½¨è¿¹æ•°æ®ï¼ˆå‘é‡åŒ–ï¼‰"""
    obs = env.reset()

    # å­˜å‚¨æ¯ä¸ªæ™ºèƒ½ä½“çš„æ•°æ®
    trajectories = {
        agent.name: {
            'obs': [],
            'actions': [],
            'log_probs': [],
            'values': [],
            'rewards': [],
            'dones': [],
        }
        for agent in env.agents
    }

    for step in range(num_steps):
        actions = []
        log_probs = []
        values = []

        # è·å–æ¯ä¸ªæ™ºèƒ½ä½“çš„åŠ¨ä½œ
        for i, agent in enumerate(env.agents):
            policy = policies[agent.name]
            obs_tensor = obs[i] if isinstance(obs[i], torch.Tensor) else torch.tensor(obs[i], dtype=torch.float32)

            action, log_prob, value = policy.actor_critic.get_action(obs_tensor)

            actions.append(action)
            log_probs.append(log_prob.detach())
            values.append(value.detach())

            # å­˜å‚¨æ•°æ®
            trajectories[agent.name]['obs'].append(obs_tensor.detach())
            trajectories[agent.name]['log_probs'].append(log_prob)
            trajectories[agent.name]['values'].append(value)

        # æ‰§è¡ŒåŠ¨ä½œ
        obs, rews, dones, info = env.step(actions)

        # å­˜å‚¨å¥–åŠ±å’Œdone
        for i, agent in enumerate(env.agents):
            trajectories[agent.name]['actions'].append(actions[i])
            trajectories[agent.name]['rewards'].append(rews[i])
            trajectories[agent.name]['dones'].append(dones[i])

    return trajectories
```

**å…³é”®ç‚¹**ï¼š
- ä½¿ç”¨32ä¸ªå¹¶è¡Œç¯å¢ƒåŒæ—¶æ”¶é›†æ•°æ®
- æ‰¹é‡å¤„ç†æ™ºèƒ½ä½“çš„åŠ¨ä½œ
- é«˜æ•ˆçš„æ•°æ®å­˜å‚¨å’Œæ£€ç´¢

## 6. å®éªŒç»“è®º

### 6.1 ä¸»è¦å‘ç°

1. **é›†ä¸­å¼è®­ç»ƒåœ¨åä½œä»»åŠ¡ä¸­å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿**
   - CPPOå’ŒMAPPOçš„æ€§èƒ½æ˜¾è‘—ä¼˜äºIPPO
   - è¯´æ˜åœ¨éœ€è¦ç´§å¯†åä½œçš„ä»»åŠ¡ä¸­ï¼Œå…¨å±€ä¿¡æ¯è‡³å…³é‡è¦

2. **MAPPOåœ¨å®ç”¨æ€§å’Œæ€§èƒ½ä¹‹é—´å–å¾—æœ€ä½³å¹³è¡¡**
   - MAPPOçš„æ€§èƒ½æ¥è¿‘CPPOï¼Œä½†æ‰§è¡Œæ—¶ä¸éœ€è¦å…¨å±€ä¿¡æ¯
   - é€‚åˆå®é™…åº”ç”¨åœºæ™¯

3. **Transportä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§**
   - å³ä½¿ç»è¿‡300æ¬¡è¿­ä»£è®­ç»ƒï¼Œç®—æ³•ä»æœªå®Œå…¨æ”¶æ•›
   - è¯´æ˜Transportä»»åŠ¡çš„å¤æ‚æ€§

### 6.2 å±€é™æ€§

1. **è®­ç»ƒæ—¶é—´ä¸è¶³**
   - 300æ¬¡è¿­ä»£å¯èƒ½ä¸è¶³ä»¥è®©ç®—æ³•å®Œå…¨æ”¶æ•›
   - å»ºè®®å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°

2. **è¶…å‚æ•°è°ƒä¼˜ä¸å¤Ÿå……åˆ†**
   - ä½¿ç”¨äº†è®ºæ–‡ä¸­çš„é»˜è®¤è¶…å‚æ•°
   - å¯èƒ½éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œè°ƒä¼˜

3. **è¯„ä¼°æŒ‡æ ‡å•ä¸€**
   - åªä½¿ç”¨äº†å¹³å‡å¥–åŠ±ä½œä¸ºè¯„ä¼°ä¿¡æ¯
   - å»ºè®®å¢åŠ æˆåŠŸç‡ã€åä½œæ•ˆç‡ç­‰æŒ‡æ ‡

### 6.3 æœªæ¥å·¥ä½œ

1. **ç®—æ³•æ”¹è¿›**
   - æ¢ç´¢æ›´å…ˆè¿›çš„MARLç®—æ³•ï¼ˆå¦‚QMIXã€VDACç­‰ï¼‰
   - å¼•å…¥é€šä¿¡æœºåˆ¶ï¼Œå¢å¼ºæ™ºèƒ½ä½“ä¹‹é—´çš„åä½œ

2. **ä»»åŠ¡æ‰©å±•**
   - å¢åŠ åŒ…è£¹æ•°é‡å’Œæ™ºèƒ½ä½“æ•°é‡
   - å¼•å…¥éšœç¢ç‰©ï¼Œå¢åŠ ä»»åŠ¡å¤æ‚åº¦

3. **è¯„ä¼°å®Œå–„**
   - å¢åŠ æ›´å¤šè¯„ä¼°æŒ‡æ ‡
   - è¿›è¡Œæ¶ˆèå®éªŒï¼Œåˆ†æå„ç»„ä»¶çš„è´¡çŒ®

## 7. æ·±å…¥åˆ†æä¸æ”¹è¿›å»ºè®®

### 7.1 CPPOç¨³å®šæ€§é—®é¢˜æ·±å…¥åˆ†æ

#### 7.1.1 é—®é¢˜ç°è±¡

CPPOç®—æ³•åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¡¨ç°å‡ºä»¥ä¸‹ä¸ç¨³å®šç‰¹å¾ï¼š

1. **æ—©æœŸå¿«é€Ÿå­¦ä¹ **ï¼ˆè¿­ä»£0-100ï¼‰ï¼š
   - å¥–åŠ±å¿«é€Ÿä¸Šå‡è‡³å³°å€¼0.3457
   - ç­–ç•¥å¿«é€Ÿæ”¶æ•›åˆ°é«˜æ€§èƒ½åŒºåŸŸ

2. **åæœŸå‰§çƒˆæ³¢åŠ¨**ï¼ˆè¿­ä»£100-300ï¼‰ï¼š
   - å¥–åŠ±åœ¨-0.5åˆ°0.3ä¹‹é—´å‰§çƒˆæ³¢åŠ¨
   - ç­–ç•¥é¢‘ç¹å´©æºƒå’Œæ¢å¤
   - æœ€ç»ˆå¥–åŠ±é™è‡³è´Ÿå€¼-0.1356

3. **ç†µå€¼å˜åŒ–**ï¼š
   - æ—©æœŸç†µå€¼è¾ƒé«˜ï¼ˆçº¦2.8-3.0ï¼‰
   - åæœŸç†µå€¼æŒç»­ä¸‹é™ï¼ˆçº¦3.1-3.3ï¼‰
   - ç­–ç•¥è¿‡æ—©æ”¶æ•›å¯¼è‡´æ¢ç´¢ä¸è¶³

#### 7.1.2 æ ¹æœ¬åŸå› åˆ†æ

**åŸå› 1ï¼šè¿‡åº¦ä¾èµ–å…¨å±€ä¿¡æ¯**
- CPPOåœ¨è®­ç»ƒå’Œæ‰§è¡Œæ—¶éƒ½ä½¿ç”¨å…¨å±€ä¿¡æ¯
- å½“ç­–ç•¥è¿‡åº¦ä¾èµ–å…¨å±€ä¿¡æ¯æ—¶ï¼Œå¯¹ç¯å¢ƒå˜åŒ–æ•æ„Ÿ
- æ‰§è¡Œæ—¶å¦‚æœå…¨å±€ä¿¡æ¯ä¸å®Œæ•´ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™

**åŸå› 2ï¼šç­–ç•¥è¿‡æ—©æ”¶æ•›**
- ç†µç³»æ•°0.01åœ¨åæœŸå¯èƒ½è¿‡å¤§
- ç­–ç•¥åœ¨è¿­ä»£100å·¦å³å°±æ”¶æ•›åˆ°ç¡®å®šæ€§ç­–ç•¥
- ç¼ºä¹è¶³å¤Ÿçš„æ¢ç´¢èƒ½åŠ›ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜

**åŸå› 3ï¼šä»·å€¼å‡½æ•°è¿‡æ‹Ÿåˆ**
- CPPOçš„Criticä½¿ç”¨å…¨å±€ä¿¡æ¯ï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®
- å½“ç­–ç•¥æ”¹å˜æ—¶ï¼Œä»·å€¼å‡½æ•°æ— æ³•å‡†ç¡®ä¼°è®¡
- å¯¼è‡´ä¼˜åŠ¿å‡½æ•°è®¡ç®—é”™è¯¯ï¼Œç­–ç•¥æ›´æ–°ä¸ç¨³å®š

**åŸå› 4ï¼šè®­ç»ƒè¿­ä»£æ¬¡æ•°ä¸è¶³**
- 300æ¬¡è¿­ä»£ä¸è¶³ä»¥è®©CPPOå……åˆ†ç¨³å®š
- è®ºæ–‡å¯èƒ½è®­ç»ƒäº†1000æ¬¡ä»¥ä¸Š
- åæœŸæ³¢åŠ¨å¯èƒ½æ˜¯å› ä¸ºè®­ç»ƒè¿˜æœªå®Œæˆ

#### 7.1.3 æ”¹è¿›æ–¹æ¡ˆ

**æ–¹æ¡ˆ1ï¼šåŠ¨æ€ç†µç³»æ•°è°ƒæ•´**
```python
# å®ç°ç†µç³»æ•°çº¿æ€§è¡°å‡
def get_entropy_coeff(iteration, max_iterations, initial_coeff=0.01, min_coeff=0.001):
    progress = iteration / max_iterations
    return initial_coeff * (1 - progress) + min_coeff * progress
```
- æ—©æœŸï¼šç†µç³»æ•°0.01ï¼Œé¼“åŠ±æ¢ç´¢
- åæœŸï¼šç†µç³»æ•°é™è‡³0.001ï¼Œé¼“åŠ±åˆ©ç”¨

**æ–¹æ¡ˆ2ï¼šå¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°**
- å°†è®­ç»ƒè¿­ä»£ä»300å¢åŠ åˆ°1000
- ç»™CPPOè¶³å¤Ÿçš„æ—¶é—´ç¨³å®šç­–ç•¥
- è§‚å¯ŸåæœŸæ˜¯å¦èƒ½å¤Ÿç¨³å®šæ”¶æ•›

**æ–¹æ¡ˆ3ï¼šè°ƒæ•´GAEå‚æ•°**
- å°†GAEå‚æ•°Î»ä»0.95æé«˜åˆ°0.97
- å‡å°‘ä¼˜åŠ¿å‡½æ•°çš„æ–¹å·®
- æé«˜ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§

**æ–¹æ¡ˆ4ï¼šå®ç°å­¦ä¹ ç‡è°ƒåº¦**
```python
# ä½™å¼¦é€€ç«å­¦ä¹ ç‡è°ƒåº¦
def get_lr(iteration, max_iterations, initial_lr=3e-4, min_lr=1e-5):
    progress = iteration / max_iterations
    return min_lr + 0.5 * (initial_lr - min_lr) * (1 + np.cos(progress * np.pi))
```
- æ—©æœŸï¼šå­¦ä¹ ç‡3e-4ï¼Œå¿«é€Ÿå­¦ä¹ 
- åæœŸï¼šå­¦ä¹ ç‡é™è‡³1e-5ï¼Œç²¾ç»†è°ƒæ•´

**æ–¹æ¡ˆ5ï¼šå¢åŠ æ­£åˆ™åŒ–**
- åœ¨ç­–ç•¥ç½‘ç»œä¸­æ·»åŠ Dropoutå±‚
- åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ L2æ­£åˆ™åŒ–
- é˜²æ­¢ç­–ç•¥è¿‡æ‹Ÿåˆè®­ç»ƒæ•°æ®

### 7.2 è¶…å‚æ•°æ•æ„Ÿæ€§åˆ†æ

#### 7.2.1 å­¦ä¹ ç‡æ•æ„Ÿæ€§

**æµ‹è¯•èŒƒå›´**ï¼š[1e-5, 3e-5, 1e-4, 3e-4, 1e-3]

**é¢„æœŸç»“æœ**ï¼š
- 1e-5ï¼šå­¦ä¹ è¿‡æ…¢ï¼Œ300æ¬¡è¿­ä»£æ— æ³•æ”¶æ•›
- 3e-5ï¼šå­¦ä¹ ç¨³å®šï¼Œä½†æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢
- 1e-4ï¼šå­¦ä¹ é€Ÿåº¦é€‚ä¸­ï¼Œç¨³å®šæ€§è‰¯å¥½ï¼ˆæ¨èï¼‰
- 3e-4ï¼šå½“å‰è®¾ç½®ï¼Œå­¦ä¹ é€Ÿåº¦å¿«ä½†å¯èƒ½ä¸ç¨³å®š
- 1e-3ï¼šå­¦ä¹ è¿‡å¿«ï¼Œç­–ç•¥å®¹æ˜“å´©æºƒ

**å»ºè®®**ï¼šå°è¯•1e-4æˆ–2e-4ï¼Œå¹³è¡¡å­¦ä¹ é€Ÿåº¦å’Œç¨³å®šæ€§

#### 7.2.2 ç†µç³»æ•°æ•æ„Ÿæ€§

**æµ‹è¯•èŒƒå›´**ï¼š[0.001, 0.005, 0.01, 0.02, 0.05]

**é¢„æœŸç»“æœ**ï¼š
- 0.001ï¼šæ¢ç´¢ä¸è¶³ï¼Œå®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜
- 0.005ï¼šæ¢ç´¢é€‚ä¸­ï¼Œç¨³å®šæ€§è‰¯å¥½ï¼ˆæ¨èï¼‰
- 0.01ï¼šå½“å‰è®¾ç½®ï¼Œæ¢ç´¢è¾ƒå¤šä½†å¯èƒ½è¿‡åº¦
- 0.02ï¼šæ¢ç´¢è¿‡å¤šï¼Œå­¦ä¹ æ•ˆç‡ä½
- 0.05ï¼šæ¢ç´¢è¿‡åº¦ï¼Œç­–ç•¥éš¾ä»¥æ”¶æ•›

**å»ºè®®**ï¼šä½¿ç”¨åŠ¨æ€ç†µç³»æ•°ï¼Œä»0.01çº¿æ€§è¡°å‡åˆ°0.001

#### 7.2.3 GAEå‚æ•°æ•æ„Ÿæ€§

**æµ‹è¯•èŒƒå›´**ï¼š[0.90, 0.95, 0.97, 0.99]

**é¢„æœŸç»“æœ**ï¼š
- 0.90ï¼šåå·®å°ï¼Œæ–¹å·®å¤§ï¼Œè®­ç»ƒä¸ç¨³å®š
- 0.95ï¼šå½“å‰è®¾ç½®ï¼Œåå·®å’Œæ–¹å·®å¹³è¡¡
- 0.97ï¼šåå·®ç¨å¤§ï¼Œæ–¹å·®å°ï¼Œè®­ç»ƒæ›´ç¨³å®šï¼ˆæ¨èï¼‰
- 0.99ï¼šåå·®å¤§ï¼Œæ–¹å·®æå°ï¼Œå¯èƒ½æ¬ æ‹Ÿåˆ

**å»ºè®®**ï¼šå°è¯•0.97ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§

#### 7.2.4 æ‰¹æ¬¡å¤§å°æ•æ„Ÿæ€§

**æµ‹è¯•èŒƒå›´**ï¼š[32, 64, 128, 256, 512]

**é¢„æœŸç»“æœ**ï¼š
- 32ï¼šæ‰¹æ¬¡å°ï¼Œæ›´æ–°é¢‘ç¹ï¼Œä½†æ¢¯åº¦ä¼°è®¡ä¸å‡†ç¡®
- 64ï¼šæ‰¹æ¬¡é€‚ä¸­ï¼Œå¹³è¡¡å‡†ç¡®æ€§å’Œæ›´æ–°é¢‘ç‡
- 128ï¼šæ‰¹æ¬¡è¾ƒå¤§ï¼Œæ¢¯åº¦ä¼°è®¡å‡†ç¡®ï¼Œä½†æ›´æ–°æ…¢
- 256ï¼šæ‰¹æ¬¡å¤§ï¼Œæ¢¯åº¦ä¼°è®¡å‡†ç¡®ï¼Œä½†å¯èƒ½æ¬ æ‹Ÿåˆ
- 512ï¼šæ‰¹æ¬¡è¿‡å¤§ï¼Œå†…å­˜æ¶ˆè€—å¤§ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ

**å»ºè®®**ï¼šå½“å‰è®¾ç½®64-128æ˜¯åˆç†çš„ï¼Œæ— éœ€è°ƒæ•´

### 7.3 ç®—æ³•æ”¹è¿›å»ºè®®

#### 7.3.1 æ”¹è¿›CPPOç¨³å®šæ€§

**æ”¹è¿›1ï¼šåŒé‡ç­–ç•¥æ›´æ–°**
```python
# åœ¨CPPOä¸­å®ç°åŒé‡ç­–ç•¥æ›´æ–°
class ImprovedCPPO:
    def __init__(self):
        self.main_policy = PolicyNetwork()
        self.target_policy = PolicyNetwork()
        self.update_frequency = 100  # æ¯100æ¬¡è¿­ä»£æ›´æ–°ä¸€æ¬¡target policy

    def update(self, iteration):
        # ä½¿ç”¨target policyè¿›è¡Œç­–ç•¥æ›´æ–°
        if iteration % self.update_frequency == 0:
            self.target_policy.load_state_dict(self.main_policy.state_dict())

        # ä½¿ç”¨target policyè®¡ç®—ä¼˜åŠ¿
        advantages = self.compute_advantages(obs, rewards, self.target_policy)

        # æ›´æ–°main policy
        self.main_policy.update(advantages)
```

**æ”¹è¿›2ï¼šä»·å€¼å‡½æ•°é›†æˆ**
```python
# ä½¿ç”¨å¤šä¸ªä»·å€¼å‡½æ•°è¿›è¡Œé›†æˆ
class EnsembleCritic:
    def __init__(self, n_critics=5):
        self.critics = [CriticNetwork() for _ in range(n_critics)]

    def forward(self, obs):
        values = [critic(obs) for critic in self.critics]
        return torch.mean(torch.stack(values), dim=0)
```

**æ”¹è¿›3ï¼šç­–ç•¥å¹³æ»‘**
```python
# åœ¨ç­–ç•¥æ›´æ–°åæ·»åŠ ç­–ç•¥å¹³æ»‘
def smooth_policy(policy, smoothing_factor=0.1):
    with torch.no_grad():
        old_params = {name: param.clone() for name, param in policy.named_parameters()}
        for name, param in policy.named_parameters():
            param.data = (1 - smoothing_factor) * param.data + \
                        smoothing_factor * old_params[name]
```

#### 7.3.2 å¢å¼ºMAPPOæ€§èƒ½

**æ”¹è¿›1ï¼šæ³¨æ„åŠ›æœºåˆ¶**
```python
# åœ¨MAPPOçš„Criticä¸­æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶
class AttentionCritic(nn.Module):
    def __init__(self, obs_dim, n_agents):
        super().__init__()
        self.attention = nn.MultiheadAttention(obs_dim, n_heads=4)
        self.critic = nn.Linear(obs_dim, 1)

    def forward(self, obs):
        # obs shape: [batch, n_agents, obs_dim]
        obs_permuted = obs.permute(1, 0, 2)  # [n_agents, batch, obs_dim]
        attended_obs, _ = self.attention(obs_permuted, obs_permuted, obs_permuted)
        attended_obs = attended_obs.permute(1, 0, 2)  # [batch, n_agents, obs_dim]
        values = self.critic(attended_obs)
        return values.mean(dim=1)  # [batch]
```

**æ”¹è¿›2ï¼šè§’è‰²è‡ªé€‚åº”**
```python
# å®ç°è§’è‰²è‡ªé€‚åº”çš„MAPPO
class RoleAwareMAPPO:
    def __init__(self, n_agents):
        self.roles = nn.Parameter(torch.randn(n_agents, role_dim))
        self.policy = PolicyNetwork(obs_dim + role_dim, action_dim)

    def get_action(self, obs, agent_id):
        role_embedding = self.roles[agent_id]
        obs_with_role = torch.cat([obs, role_embedding], dim=-1)
        return self.policy.get_action(obs_with_role)
```

#### 7.3.3 æå‡IPPOåä½œèƒ½åŠ›

**æ”¹è¿›1ï¼šé€šä¿¡æœºåˆ¶**
```python
# åœ¨IPPOä¸­æ·»åŠ é€šä¿¡æœºåˆ¶
class CommIPPO:
    def __init__(self, n_agents, comm_dim=32):
        self.comm_channels = nn.Parameter(torch.randn(n_agents, n_agents, comm_dim))
        self.policy = PolicyNetwork(obs_dim + comm_dim, action_dim)

    def get_comm_message(self, obs, agent_id):
        # æ¥æ”¶å…¶ä»–æ™ºèƒ½ä½“çš„é€šä¿¡æ¶ˆæ¯
        messages = []
        for other_agent_id in range(self.n_agents):
            if other_agent_id != agent_id:
                channel = self.comm_channels[other_agent_id, agent_id]
                messages.append(obs[other_agent_id] @ channel)
        return torch.mean(torch.stack(messages), dim=0)

    def get_action(self, obs, agent_id):
        comm_msg = self.get_comm_message(obs, agent_id)
        obs_with_comm = torch.cat([obs[agent_id], comm_msg], dim=-1)
        return self.policy.get_action(obs_with_comm)
```

**æ”¹è¿›2ï¼šè§’è‰²åˆ†å·¥**
```python
# å®ç°è§’è‰²åˆ†å·¥çš„IPPO
class RoleDivisionIPPO:
    def __init__(self, n_agents):
        self.n_roles = 2  # æ¨åŠ¨è€…ã€ç¨³å®šè€…
        self.role_assignment = nn.Parameter(torch.randn(n_agents, self.n_roles))
        self.policies = nn.ModuleList([PolicyNetwork(obs_dim, action_dim)
                                      for _ in range(self.n_roles)])

    def get_action(self, obs, agent_id):
        role_probs = torch.softmax(self.role_assignment[agent_id], dim=0)
        role_id = torch.multinomial(role_probs, 1).item()
        return self.policies[role_id].get_action(obs[agent_id])
```

### 7.4 å®éªŒè®¾è®¡å»ºè®®

#### 7.4.1 æ¶ˆèå®éªŒ

**å®éªŒ1ï¼šç†µç³»æ•°çš„å½±å“**
- å›ºå®šå…¶ä»–è¶…å‚æ•°ï¼Œæµ‹è¯•ä¸åŒç†µç³»æ•°
- å¯¹æ¯”å­¦ä¹ æ›²çº¿ã€æœ€ç»ˆæ€§èƒ½ã€ç¨³å®šæ€§
- ç¡®å®šæœ€ä¼˜ç†µç³»æ•°å€¼

**å®éªŒ2ï¼šGAEå‚æ•°çš„å½±å“**
- å›ºå®šå…¶ä»–è¶…å‚æ•°ï¼Œæµ‹è¯•ä¸åŒGAEå‚æ•°
- å¯¹æ¯”ä¼˜åŠ¿å‡½æ•°çš„æ–¹å·®å’Œåå·®
- ç¡®å®šæœ€ä¼˜GAEå‚æ•°å€¼

**å®éªŒ3ï¼šç½‘ç»œæ·±åº¦çš„å½±å“**
- æµ‹è¯•ä¸åŒç½‘ç»œæ·±åº¦[1, 2, 3, 4]å±‚
- å¯¹æ¯”è®­ç»ƒæ—¶é—´ã€æ€§èƒ½ã€ç¨³å®šæ€§
- ç¡®å®šæœ€ä¼˜ç½‘ç»œæ¶æ„

#### 7.4.2 é²æ£’æ€§æµ‹è¯•

**æµ‹è¯•1ï¼šä¸åŒéšæœºç§å­**
- ä½¿ç”¨5ä¸ªä¸åŒçš„éšæœºç§å­è®­ç»ƒ
- è®¡ç®—å¹³å‡æ€§èƒ½å’Œæ ‡å‡†å·®
- è¯„ä¼°ç®—æ³•çš„é²æ£’æ€§

**æµ‹è¯•2ï¼šä¸åŒç¯å¢ƒå‚æ•°**
- ä¿®æ”¹åŒ…è£¹è´¨é‡[25, 50, 75, 100]
- ä¿®æ”¹æ™ºèƒ½ä½“æ•°é‡[2, 4, 6, 8]
- è¯„ä¼°ç®—æ³•çš„æ³›åŒ–èƒ½åŠ›

**æµ‹è¯•3ï¼šå™ªå£°å¹²æ‰°**
- åœ¨è§‚æµ‹ä¸­æ·»åŠ é«˜æ–¯å™ªå£°
- åœ¨åŠ¨ä½œä¸­æ·»åŠ æ‰§è¡Œå™ªå£°
- è¯„ä¼°ç®—æ³•çš„æŠ—å¹²æ‰°èƒ½åŠ›

#### 7.4.3 æ‰©å±•å®éªŒ

**å®éªŒ1ï¼šå¤šåŒ…è£¹ä»»åŠ¡**
- å°†åŒ…è£¹æ•°é‡ä»1å¢åŠ åˆ°[2, 3, 5]
- è¯„ä¼°ç®—æ³•åœ¨æ›´å¤æ‚ä»»åŠ¡ä¸­çš„è¡¨ç°
- åˆ†æåä½œç­–ç•¥çš„æ‰©å±•æ€§

**å®éªŒ2ï¼šåŠ¨æ€ç¯å¢ƒ**
- åœ¨ç¯å¢ƒä¸­æ·»åŠ ç§»åŠ¨éšœç¢ç‰©
- è¯„ä¼°ç®—æ³•çš„é€‚åº”èƒ½åŠ›
- æµ‹è¯•åœ¨çº¿å­¦ä¹ èƒ½åŠ›

**å®éªŒ3ï¼šéƒ¨åˆ†å¯è§‚æµ‹æ€§**
- é™åˆ¶æ™ºèƒ½ä½“çš„è§‚æµ‹èŒƒå›´
- è¯„ä¼°ç®—æ³•åœ¨éƒ¨åˆ†å¯è§‚æµ‹ç¯å¢ƒä¸­çš„è¡¨ç°
- æµ‹è¯•é€šä¿¡æœºåˆ¶çš„æœ‰æ•ˆæ€§

### 7.5 å®æ–½ä¼˜å…ˆçº§

**é«˜ä¼˜å…ˆçº§ï¼ˆç«‹å³å®æ–½ï¼‰**ï¼š
1. âœ… å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°è‡³1000æ¬¡
2. âœ… å®ç°åŠ¨æ€ç†µç³»æ•°è°ƒæ•´
3. âœ… è°ƒæ•´GAEå‚æ•°è‡³0.97
4. âœ… è¿›è¡Œå¤šæ¬¡å®éªŒå–å¹³å‡

**ä¸­ä¼˜å…ˆçº§ï¼ˆçŸ­æœŸå®æ–½ï¼‰**ï¼š
1. â³ å®ç°å­¦ä¹ ç‡è°ƒåº¦
2. â³ æ·»åŠ æ³¨æ„åŠ›æœºåˆ¶åˆ°MAPPO
3. â³ è¿›è¡Œæ¶ˆèå®éªŒ
4. â³ æµ‹è¯•ä¸åŒè¶…å‚æ•°ç»„åˆ

**ä½ä¼˜å…ˆçº§ï¼ˆé•¿æœŸç ”ç©¶ï¼‰**ï¼š
1. â³ å®ç°é€šä¿¡æœºåˆ¶
2. â³ ç ”ç©¶è§’è‰²è‡ªé€‚åº”
3. â³ æ‰©å±•åˆ°å¤šåŒ…è£¹ä»»åŠ¡
4. â³ æµ‹è¯•åŠ¨æ€ç¯å¢ƒ

### 7.6 é¢„æœŸæ”¹è¿›æ•ˆæœ

**CPPOæ”¹è¿›åé¢„æœŸ**ï¼š
- å³°å€¼å¥–åŠ±ï¼š0.3457 â†’ 0.4000ï¼ˆ+15.7%ï¼‰
- æœ€ç»ˆå¥–åŠ±ï¼š-0.1356 â†’ 0.2000ï¼ˆ+247.6%ï¼‰
- ç¨³å®šæ€§ï¼šå¤§å¹…æ”¹å–„ï¼Œæ³¢åŠ¨å‡å°‘80%

**MAPPOæ”¹è¿›åé¢„æœŸ**ï¼š
- å³°å€¼å¥–åŠ±ï¼š0.2976 â†’ 0.3500ï¼ˆ+17.6%ï¼‰
- æœ€ç»ˆå¥–åŠ±ï¼š0.0381 â†’ 0.1500ï¼ˆ+293.7%ï¼‰
- æ”¶æ•›é€Ÿåº¦ï¼šæå‡30%

**IPPOæ”¹è¿›åé¢„æœŸ**ï¼š
- å³°å€¼å¥–åŠ±ï¼š0.1458 â†’ 0.2000ï¼ˆ+37.2%ï¼‰
- æœ€ç»ˆå¥–åŠ±ï¼š-0.0477 â†’ 0.0500ï¼ˆ+204.8%ï¼‰
- åä½œèƒ½åŠ›ï¼šæ˜¾è‘—æå‡

## 8. ç®—æ³•æ”¹è¿›å®æ–½

### 8.1 æ”¹è¿›å®æ–½æ¦‚è¿°

åŸºäºæœ¬ç« çš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å·²ç»å®æ–½äº†é«˜ä¼˜å…ˆçº§çš„ç®—æ³•æ”¹è¿›ï¼Œä»¥æå‡CPPOã€MAPPOã€IPPOä¸‰ç§ç®—æ³•åœ¨Transportä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚

### 8.2 å·²å®æ–½çš„æ”¹è¿›

#### âœ… æ”¹è¿›1ï¼šåŠ¨æ€ç†µç³»æ•°è°ƒæ•´

**å®ç°æ–¹å¼**ï¼š
- é€šè¿‡`DynamicEntropyCallback`å›è°ƒç±»å®ç°
- çº¿æ€§è¡°å‡ï¼šä»0.01è¡°å‡åˆ°0.001

**æŠ€æœ¯ç»†èŠ‚**ï¼š
```python
class DynamicEntropyCallback(BaseCallback):
    def __init__(self, initial_ent_coef=0.01, min_ent_coef=0.001):
        super().__init__()
        self.initial_ent_coef = initial_ent_coef
        self.min_ent_coef = min_ent_coef

    def _on_step(self):
        progress = self.num_timesteps / self.total_timesteps
        current_ent_coef = self.initial_ent_coef * (1 - progress) + self.min_ent_coef * progress
        self.model.ent_coef = current_ent_coef
        return True
```

**é¢„æœŸæ•ˆæœ**ï¼š
- æ—©æœŸï¼šé¼“åŠ±æ¢ç´¢ï¼Œé¿å…è¿‡æ—©æ”¶æ•›
- åæœŸï¼šé¼“åŠ±åˆ©ç”¨ï¼Œæé«˜ç­–ç•¥ç¡®å®šæ€§
- æ•´ä½“ï¼šæé«˜è®­ç»ƒç¨³å®šæ€§ï¼Œå‡å°‘ç­–ç•¥å´©æºƒ

#### âœ… æ”¹è¿›2ï¼šè°ƒæ•´GAEå‚æ•°

**å‚æ•°è°ƒæ•´**ï¼š
- åŸå§‹å€¼ï¼š0.95
- æ”¹è¿›å€¼ï¼š0.97

**é¢„æœŸæ•ˆæœ**ï¼š
- å‡å°‘ä¼˜åŠ¿å‡½æ•°çš„æ–¹å·®
- æé«˜ç­–ç•¥æ›´æ–°çš„ç¨³å®šæ€§
- æ”¹å–„ä»·å€¼å‡½æ•°ä¼°è®¡å‡†ç¡®æ€§

#### âœ… æ”¹è¿›3ï¼šé™ä½å­¦ä¹ ç‡

**å‚æ•°è°ƒæ•´**ï¼š
- åŸå§‹å€¼ï¼š3e-4
- æ”¹è¿›å€¼ï¼š2e-4

**é¢„æœŸæ•ˆæœ**ï¼š
- æé«˜è®­ç»ƒç¨³å®šæ€§
- å‡å°‘ç­–ç•¥å´©æºƒ
- å¹³è¡¡å­¦ä¹ é€Ÿåº¦å’Œç¨³å®šæ€§

#### âœ… æ”¹è¿›4ï¼šå¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°

**å‚æ•°è°ƒæ•´**ï¼š
- åŸå§‹å€¼ï¼š300æ¬¡è¿­ä»£
- æ”¹è¿›å€¼ï¼š1000æ¬¡è¿­ä»£ï¼ˆé»˜è®¤ï¼‰

**é¢„æœŸæ•ˆæœ**ï¼š
- ç»™ç®—æ³•å……åˆ†çš„æ—¶é—´æ”¶æ•›
- è®©CPPOæœ‰è¶³å¤Ÿçš„æ—¶é—´ç¨³å®š
- æé«˜æœ€ç»ˆæ€§èƒ½

#### âœ… æ”¹è¿›5ï¼šè‡ªåŠ¨æ£€æŸ¥ç‚¹ä¿å­˜

**å®ç°æ–¹å¼**ï¼š
- é€šè¿‡`CheckpointCallback`å®ç°
- æ¯200æ¬¡è¿­ä»£è‡ªåŠ¨ä¿å­˜

**é¢„æœŸæ•ˆæœ**ï¼š
- é˜²æ­¢è®­ç»ƒä¸­æ–­å¯¼è‡´çš„æ•°æ®ä¸¢å¤±
- ä¾¿äºé€‰æ‹©æœ€ä½³æ¨¡å‹
- æ”¯æŒè®­ç»ƒæ¢å¤

#### âœ… æ”¹è¿›6ï¼šè¯¦ç»†çš„æŒ‡æ ‡è®°å½•

**å®ç°æ–¹å¼**ï¼š
- é€šè¿‡`MetricsCallback`å®ç°
- è®°å½•å¥–åŠ±ã€æŸå¤±ã€è¶…å‚æ•°ç­‰

**é¢„æœŸæ•ˆæœ**ï¼š
- ä¾¿äºåˆ†æå’Œè°ƒè¯•è®­ç»ƒè¿‡ç¨‹
- æ”¯æŒæ€§èƒ½å¯¹æ¯”
- ç”Ÿæˆè¯¦ç»†çš„è®­ç»ƒæŠ¥å‘Š

### 8.3 æ–°å¢æ–‡ä»¶

#### 1. æ”¹è¿›çš„è®­ç»ƒè„šæœ¬
- **æ–‡ä»¶**ï¼š`marl_algorithms/scripts/train_improved.py`
- **åŠŸèƒ½**ï¼šå®æ–½æ‰€æœ‰æ”¹è¿›çš„è®­ç»ƒè„šæœ¬
- **ç‰¹æ€§**ï¼š
  - åŠ¨æ€ç†µç³»æ•°è°ƒæ•´
  - ä¼˜åŒ–çš„è¶…å‚æ•°é…ç½®
  - è‡ªåŠ¨æ£€æŸ¥ç‚¹ä¿å­˜
  - è¯¦ç»†çš„æŒ‡æ ‡è®°å½•

#### 2. å¿«é€Ÿæµ‹è¯•è„šæœ¬
- **æ–‡ä»¶**ï¼š`marl_algorithms/scripts/quick_test.py`
- **åŠŸèƒ½**ï¼šå¿«é€ŸéªŒè¯æ”¹è¿›æ•ˆæœ
- **ç”¨é€”**ï¼š
  - åœ¨å®Œæ•´è®­ç»ƒå‰éªŒè¯æ”¹è¿›æ˜¯å¦æœ‰æ•ˆ
  - å¿«é€Ÿå¯¹æ¯”ä¸åŒé…ç½®
  - è°ƒè¯•è¶…å‚æ•°

#### 3. å¯¹æ¯”è¯„ä¼°è„šæœ¬
- **æ–‡ä»¶**ï¼š`marl_algorithms/scripts/compare_improvements.py`
- **åŠŸèƒ½**ï¼šå¯¹æ¯”åŸå§‹ç®—æ³•å’Œæ”¹è¿›ç®—æ³•çš„æ€§èƒ½
- **è¾“å‡º**ï¼š
  - æ§åˆ¶å°æ€§èƒ½å¯¹æ¯”
  - JSONæ ¼å¼è¯¦ç»†ç»“æœ
  - æ€§èƒ½å¯¹æ¯”æŸ±çŠ¶å›¾

#### 4. ä½¿ç”¨æŒ‡å—
- **æ–‡ä»¶**ï¼š`marl_algorithms/IMPROVEMENTS_GUIDE.md`
- **å†…å®¹**ï¼š
  - æ”¹è¿›å†…å®¹è¯¦è§£
  - ä½¿ç”¨æ–¹æ³•
  - æ–‡ä»¶ç»“æ„
  - é¢„æœŸæ•ˆæœ
  - ç›‘æ§æ–¹æ³•
  - å¸¸è§é—®é¢˜

#### 5. æ”¹è¿›æ€»ç»“æ–‡æ¡£
- **æ–‡ä»¶**ï¼š`marl_algorithms/IMPROVEMENTS_SUMMARY.md`
- **å†…å®¹**ï¼š
  - æ”¹è¿›æ¦‚è¿°
  - å®æ–½çš„æ”¹è¿›
  - æ–°å¢æ–‡ä»¶
  - ä½¿ç”¨æµç¨‹
  - é¢„æœŸæ•ˆæœ
  - ä¸‹ä¸€æ­¥è®¡åˆ’

### 8.4 ä½¿ç”¨æµç¨‹

#### ç¬¬ä¸€æ­¥ï¼šå¿«é€Ÿæµ‹è¯•ï¼ˆæ¨èï¼‰

```bash
# æ¿€æ´»ç¯å¢ƒ
source /root/RL_Assignment/venv/bin/activate

# å¿«é€Ÿæµ‹è¯•MAPPOï¼ˆä»…è®­ç»ƒ50æ¬¡è¿­ä»£ï¼‰
python /root/RL_Assignment/marl_algorithms/scripts/quick_test.py \
    --algorithm MAPPO \
    --iterations 50
```

**ç›®çš„**ï¼šéªŒè¯æ”¹è¿›æ˜¯å¦æœ‰æ•ˆï¼Œé¿å…æµªè´¹æ—¶é—´è¿›è¡Œæ— æ•ˆçš„å®Œæ•´è®­ç»ƒ

#### ç¬¬äºŒæ­¥ï¼šå®Œæ•´è®­ç»ƒ

```bash
# è®­ç»ƒæ”¹è¿›çš„MAPPOï¼ˆ1000æ¬¡è¿­ä»£ï¼‰
python /root/RL_Assignment/marl_algorithms/scripts/train_improved.py \
    --algorithm MAPPO \
    --iterations 1000
```

**ç›®çš„**ï¼šå……åˆ†è®­ç»ƒç®—æ³•ï¼Œè·å¾—æœ€ä½³æ€§èƒ½

#### ç¬¬ä¸‰æ­¥ï¼šå¯¹æ¯”è¯„ä¼°

```bash
# å¯¹æ¯”æ‰€æœ‰ç®—æ³•
python /root/RL_Assignment/marl_algorithms/scripts/compare_improvements.py \
    --algorithms CPPO MAPPO IPPO \
    --episodes 10
```

**ç›®çš„**ï¼šè¯„ä¼°æ”¹è¿›æ•ˆæœï¼Œç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š

### 8.5 æ”¹è¿›é…ç½®å¯¹æ¯”

| å‚æ•° | åŸå§‹é…ç½® | æ”¹è¿›é…ç½® | æ”¹è¿›è¯´æ˜ |
|------|----------|----------|----------|
| å­¦ä¹ ç‡ | 3e-4 | 2e-4 | é™ä½ä»¥æé«˜ç¨³å®šæ€§ |
| GAEå‚æ•° | 0.95 | 0.97 | æé«˜ä»¥å‡å°‘æ–¹å·® |
| ç†µç³»æ•° | 0.01ï¼ˆå›ºå®šï¼‰ | 0.01â†’0.001ï¼ˆåŠ¨æ€ï¼‰ | çº¿æ€§è¡°å‡ |
| è®­ç»ƒè¿­ä»£ | 300 | 1000 | å¢åŠ ä»¥å……åˆ†æ”¶æ•› |
| æ£€æŸ¥ç‚¹ä¿å­˜ | ä»…æœ€ç»ˆ | æ¯200æ¬¡ | é˜²æ­¢æ•°æ®ä¸¢å¤± |
| æŒ‡æ ‡è®°å½• | åŸºç¡€ | è¯¦ç»† | ä¾¿äºåˆ†æè°ƒè¯• |

### 8.6 é¢„æœŸæ”¹è¿›æ•ˆæœæ€»ç»“

æ ¹æ®æ·±å…¥åˆ†æï¼Œé¢„æœŸæ”¹è¿›æ•ˆæœå¦‚ä¸‹ï¼š

#### CPPO
| æŒ‡æ ‡ | åŸå§‹å€¼ | é¢„æœŸå€¼ | æ”¹è¿›å¹…åº¦ |
|------|--------|--------|----------|
| å³°å€¼å¥–åŠ± | 0.3457 | 0.4000 | +15.7% |
| æœ€ç»ˆå¥–åŠ± | -0.1356 | 0.2000 | +247.6% |
| ç¨³å®šæ€§ | æ³¢åŠ¨å¤§ | å¤§å¹…æ”¹å–„ | æ³¢åŠ¨å‡å°‘80% |

**å…³é”®æ”¹è¿›**ï¼šåŠ¨æ€ç†µç³»æ•°å’Œé™ä½å­¦ä¹ ç‡åº”è¯¥èƒ½æ˜¾è‘—æ”¹å–„CPPOçš„ç¨³å®šæ€§é—®é¢˜ã€‚

#### MAPPO
| æŒ‡æ ‡ | åŸå§‹å€¼ | é¢„æœŸå€¼ | æ”¹è¿›å¹…åº¦ |
|------|--------|--------|----------|
| å³°å€¼å¥–åŠ± | 0.2976 | 0.3500 | +17.6% |
| æœ€ç»ˆå¥–åŠ± | 0.0381 | 0.1500 | +293.7% |
| æ”¶æ•›é€Ÿåº¦ | ä¸­ç­‰ | æå‡30% | +30% |

**å…³é”®æ”¹è¿›**ï¼šå¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°å’Œä¼˜åŒ–è¶…å‚æ•°åº”è¯¥èƒ½æå‡MAPPOçš„æœ€ç»ˆæ€§èƒ½ã€‚

#### IPPO
| æŒ‡æ ‡ | åŸå§‹å€¼ | é¢„æœŸå€¼ | æ”¹è¿›å¹…åº¦ |
|------|--------|--------|----------|
| å³°å€¼å¥–åŠ± | 0.1458 | 0.2000 | +37.2% |
| æœ€ç»ˆå¥–åŠ± | -0.0477 | 0.0500 | +204.8% |
| åä½œèƒ½åŠ› | è¾ƒå·® | æ˜¾è‘—æå‡ | è´¨çš„é£è·ƒ |

**å…³é”®æ”¹è¿›**ï¼šæ‰€æœ‰æ”¹è¿›éƒ½åº”è¯¥èƒ½æå‡IPPOçš„æ€§èƒ½ï¼Œä½†IPPOçš„å±€é™æ€§ä»ç„¶å­˜åœ¨ã€‚

### 8.7 ä¸‹ä¸€æ­¥è®¡åˆ’

#### çŸ­æœŸè®¡åˆ’ï¼ˆ1-2å‘¨ï¼‰

1. âœ… **å®ŒæˆåŸºç¡€æ”¹è¿›**ï¼ˆå·²å®Œæˆï¼‰
   - åŠ¨æ€ç†µç³»æ•°è°ƒæ•´
   - è°ƒæ•´GAEå‚æ•°
   - é™ä½å­¦ä¹ ç‡
   - å¢åŠ è®­ç»ƒè¿­ä»£

2. ğŸ”„ **éªŒè¯æ”¹è¿›æ•ˆæœ**ï¼ˆè¿›è¡Œä¸­ï¼‰
   - è¿è¡Œå¿«é€Ÿæµ‹è¯•
   - å®Œæ•´è®­ç»ƒæ‰€æœ‰ç®—æ³•
   - å¯¹æ¯”è¯„ä¼°æ€§èƒ½

3. â³ **åˆ†æç»“æœ**ï¼ˆå¾…å¼€å§‹ï¼‰
   - åˆ†æè®­ç»ƒæ›²çº¿
   - è¯„ä¼°æ”¹è¿›å¹…åº¦
   - æ›´æ–°å®éªŒæŠ¥å‘Š

#### ä¸­æœŸè®¡åˆ’ï¼ˆ3-4å‘¨ï¼‰

4. â³ **å®æ–½è¿›é˜¶æ”¹è¿›**ï¼ˆå¾…å¼€å§‹ï¼‰
   - å­¦ä¹ ç‡è°ƒåº¦
   - æ³¨æ„åŠ›æœºåˆ¶
   - ä»·å€¼å‡½æ•°é›†æˆ

5. â³ **æ¶ˆèå®éªŒ**ï¼ˆå¾…å¼€å§‹ï¼‰
   - æµ‹è¯•æ¯ä¸ªæ”¹è¿›çš„ç‹¬ç«‹è´¡çŒ®
   - ç¡®å®šæœ€ä¼˜å‚æ•°ç»„åˆ

#### é•¿æœŸè®¡åˆ’ï¼ˆ1-2ä¸ªæœˆï¼‰

6. â³ **ç®—æ³•åˆ›æ–°**ï¼ˆå¾…å¼€å§‹ï¼‰
   - é€šä¿¡æœºåˆ¶
   - è§’è‰²è‡ªé€‚åº”
   - å±‚æ¬¡åŒ–MARL

7. â³ **ä»»åŠ¡æ‰©å±•**ï¼ˆå¾…å¼€å§‹ï¼‰
   - å¤šåŒ…è£¹ä»»åŠ¡
   - åŠ¨æ€ç¯å¢ƒ
   - éƒ¨åˆ†å¯è§‚æµ‹æ€§

### 8.8 ç›‘æ§å’Œè°ƒè¯•

#### TensorBoardç›‘æ§

```bash
tensorboard --logdir /root/RL_Assignment/marl_algorithms/results
```

**å…³é”®æŒ‡æ ‡**ï¼š
- `rollout/ep_rew_mean`ï¼šå¹³å‡å¥–åŠ±ï¼ˆåº”è¯¥ç¨³æ­¥ä¸Šå‡ï¼‰
- `train/learning_rate`ï¼šå­¦ä¹ ç‡ï¼ˆå›ºå®šï¼‰
- `train/ent_coef`ï¼šç†µç³»æ•°ï¼ˆåº”è¯¥çº¿æ€§ä¸‹é™ï¼‰
- `train/policy_loss`ï¼šç­–ç•¥æŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
- `train/value_loss`ï¼šä»·å€¼æŸå¤±ï¼ˆåº”è¯¥ä¸‹é™ï¼‰

#### æ—¥å¿—åˆ†æ

è®­ç»ƒè„šæœ¬ä¼šè¾“å‡ºè¯¦ç»†çš„æ—¥å¿—ï¼š
```
Step 0: ent_coef = 0.010000
Step 10000: ent_coef = 0.009000
Step 20000: ent_coef = 0.008000
...
```

**æ­£å¸¸æƒ…å†µ**ï¼š
- ç†µç³»æ•°åº”è¯¥çº¿æ€§ä¸‹é™
- å¥–åŠ±åº”è¯¥ç¨³æ­¥ä¸Šå‡
- æŸå¤±åº”è¯¥é€æ¸ä¸‹é™

**å¼‚å¸¸æƒ…å†µ**ï¼š
- å¥–åŠ±çªç„¶ä¸‹é™ï¼šå¯èƒ½å­¦ä¹ ç‡è¿‡å¤§
- ç­–ç•¥å´©æºƒï¼šæ£€æŸ¥æ¢¯åº¦è£å‰ªå’ŒGAEå‚æ•°
- æ”¶æ•›è¿‡æ…¢ï¼šå¯èƒ½éœ€è¦æé«˜å­¦ä¹ ç‡

### 8.9 æ³¨æ„äº‹é¡¹

#### è®­ç»ƒæ—¶é—´

- **å¿«é€Ÿæµ‹è¯•**ï¼šçº¦5-10åˆ†é’Ÿ
- **å®Œæ•´è®­ç»ƒ**ï¼šçº¦40-50åˆ†é’Ÿï¼ˆæ¯ä¸ªç®—æ³•ï¼‰
- **å¯¹æ¯”è¯„ä¼°**ï¼šçº¦5-10åˆ†é’Ÿ

#### ç¡¬ä»¶è¦æ±‚

- **CPU**ï¼šå»ºè®®4æ ¸ä»¥ä¸Š
- **å†…å­˜**ï¼šå»ºè®®8GBä»¥ä¸Š
- **ç£ç›˜**ï¼šå»ºè®®10GBä»¥ä¸Šï¼ˆç”¨äºä¿å­˜æ¨¡å‹å’Œæ—¥å¿—ï¼‰

#### ç¯å¢ƒè¦æ±‚

- Python 3.10+
- PyTorch 2.0+
- Stable-Baselines3 2.0+
- VMAS 1.5+

### 8.10 å¸¸è§é—®é¢˜

#### Q1: æ”¹è¿›åæ€§èƒ½åè€Œä¸‹é™äº†ï¼Ÿ

**A**: å¯èƒ½çš„åŸå› ï¼š
1. è¶…å‚æ•°ä¸é€‚åˆå½“å‰ç¯å¢ƒ
2. éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´
3. éšæœºç§å­å½±å“

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å°è¯•è°ƒæ•´è¶…å‚æ•°
2. å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°
3. å¤šæ¬¡å®éªŒå–å¹³å‡

#### Q2: è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°NaNï¼Ÿ

**A**: å¯èƒ½çš„åŸå› ï¼š
1. å­¦ä¹ ç‡è¿‡å¤§
2. æ¢¯åº¦çˆ†ç‚¸
3. æ•°å€¼ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. é™ä½å­¦ä¹ ç‡
2. å¢åŠ æ¢¯åº¦è£å‰ª
3. æ£€æŸ¥è§‚æµ‹å’Œå¥–åŠ±èŒƒå›´

#### Q3: å¦‚ä½•é€‰æ‹©æœ€ä½³æ£€æŸ¥ç‚¹ï¼Ÿ

**A**: æ¨èæ–¹æ³•ï¼š
1. é€‰æ‹©éªŒè¯é›†æ€§èƒ½æœ€å¥½çš„
2. æˆ–é€‰æ‹©è®­ç»ƒåæœŸçš„ï¼ˆå¦‚800-1000æ¬¡è¿­ä»£ï¼‰
3. ä½¿ç”¨å¯¹æ¯”è¯„ä¼°è„šæœ¬è¯„ä¼°æ¯ä¸ªæ£€æŸ¥ç‚¹

### 8.11 æ€»ç»“

æˆ‘ä»¬å·²ç»æˆåŠŸå®æ–½äº†é«˜ä¼˜å…ˆçº§çš„ç®—æ³•æ”¹è¿›ï¼ŒåŒ…æ‹¬ï¼š

1. âœ… åŠ¨æ€ç†µç³»æ•°è°ƒæ•´
2. âœ… è°ƒæ•´GAEå‚æ•°è‡³0.97
3. âœ… é™ä½å­¦ä¹ ç‡è‡³2e-4
4. âœ… å¢åŠ è®­ç»ƒè¿­ä»£æ¬¡æ•°è‡³1000
5. âœ… è‡ªåŠ¨æ£€æŸ¥ç‚¹ä¿å­˜
6. âœ… è¯¦ç»†çš„æŒ‡æ ‡è®°å½•

è¿™äº›æ”¹è¿›åº”è¯¥èƒ½æ˜¾è‘—æå‡CPPOã€MAPPOã€IPPOä¸‰ç§ç®—æ³•åœ¨Transportä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯æ”¹å–„CPPOçš„ç¨³å®šæ€§é—®é¢˜ã€‚

ä¸‹ä¸€æ­¥æ˜¯è¿è¡Œå¿«é€Ÿæµ‹è¯•éªŒè¯æ”¹è¿›æ•ˆæœï¼Œç„¶åè¿›è¡Œå®Œæ•´è®­ç»ƒå’Œå¯¹æ¯”è¯„ä¼°ã€‚

## 7. é™„å½•

### 7.1 å®éªŒç¯å¢ƒ

- **æ“ä½œç³»ç»Ÿ**ï¼šLinux 6.6.87.2-microsoft-standard-WSL2
- **Pythonç‰ˆæœ¬**ï¼š3.10
- **PyTorchç‰ˆæœ¬**ï¼š2.9.1+cpu
- **VMASç‰ˆæœ¬**ï¼š1.5.2ï¼ˆæœ¬åœ°ç‰ˆæœ¬ï¼‰

### 7.2 è®­ç»ƒå‘½ä»¤

```bash
# è®­ç»ƒMAPPO
python /root/RL_Assignment/marl_algorithms/scripts/train_vmas.py \
    --algorithm MAPPO \
    --iterations 300 \
    --steps 200 \
    --envs 32

# è®­ç»ƒCPPO
python /root/RL_Assignment/marl_algorithms/scripts/train_vmas.py \
    --algorithm CPPO \
    --iterations 300 \
    --steps 200 \
    --envs 32

# è®­ç»ƒIPPO
python /root/RL_Assignment/marl_algorithms/scripts/train_vmas.py \
    --algorithm IPPO \
    --iterations 300 \
    --steps 200 \
    --envs 32
```

### 7.3 ç»“æœæ–‡ä»¶

**è®­ç»ƒç»“æœJSONæ–‡ä»¶**ï¼š
- `/root/RL_Assignment/marl_algorithms/results/MAPPO/MAPPO_transport_2026-01-15_11-44-53.json`
- `/root/RL_Assignment/marl_algorithms/results/CPPO/CPPO_transport_2026-01-15_12-00-19.json`
- `/root/RL_Assignment/marl_algorithms/results/IPPO/IPPO_transport_2026-01-15_12-14-38.json`

**è®­ç»ƒæ›²çº¿å›¾ç‰‡**ï¼š
- `/root/RL_Assignment/marl_algorithms/results/MAPPO/MAPPO_transport_2026-01-15_11-44-53.png`
- `/root/RL_Assignment/marl_algorithms/results/CPPO/CPPO_transport_2026-01-15_12-00-19.png`
- `/root/RL_Assignment/marl_algorithms/results/IPPO/IPPO_transport_2026-01-15_12-14-38.png`

**æ¨¡å‹æ£€æŸ¥ç‚¹**ï¼š
- `/root/RL_Assignment/marl_algorithms/checkpoints/MAPPO/MAPPO_model.pth`
- `/root/RL_Assignment/marl_algorithms/checkpoints/CPPO/CPPO_model.pth`
- `/root/RL_Assignment/marl_algorithms/checkpoints/IPPO/agent_0_model.pth` ~ `agent_3_model.pth`

### 7.4 å‚è€ƒæ–‡çŒ®

1. Bettini, M., et al. "VMAS: A Vectorized Multi-Agent Simulator for Collective Robot Learning." arXiv preprint arXiv:2207.03530 (2022).

2. Schulman, J., et al. "Proximal Policy Optimization Algorithms." arXiv preprint arXiv:1707.06347 (2017).

3. Lowe, R., et al. "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments." NeurIPS 2017.

---

**æŠ¥å‘Šæ—¥æœŸ**ï¼š2026å¹´1æœˆ15æ—¥
**ä½œè€…**ï¼šé™ˆä¿Šå¸†
**ç‰ˆæœ¬**ï¼š1.0